{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022244,
     "end_time": "2021-04-16T08:44:07.957930",
     "exception": false,
     "start_time": "2021-04-16T08:44:07.935686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Installing Libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-16T08:44:08.004670Z",
     "iopub.status.busy": "2021-04-16T08:44:08.003877Z",
     "iopub.status.idle": "2021-04-16T08:44:42.698283Z",
     "shell.execute_reply": "2021-04-16T08:44:42.697378Z"
    },
    "papermill": {
     "duration": 34.720516,
     "end_time": "2021-04-16T08:44:42.698408",
     "exception": false,
     "start_time": "2021-04-16T08:44:07.977892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.6.0)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (0.23.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.18.5)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (1.4.1)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet) (4.45.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet) (0.18.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet) (0.14.1)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-2.0.0\r\n",
      "Processing /kaggle/input/iterative-stratification/iterative-stratification-master\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.18.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.4.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (0.14.1)\r\n",
      "Building wheels for collected packages: iterative-stratification\r\n",
      "  Building wheel for iterative-stratification (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for iterative-stratification: filename=iterative_stratification-0.1.6-py3-none-any.whl size=8401 sha256=24820373b51c58fd34ba153b41319d41f8fffd227bde3d7817760a4c4d458448\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/47/3f/eb4af42d124f37d23d6f13a4c8bbc32c1d70140e6e1cecb4aa\r\n",
      "Successfully built iterative-stratification\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n"
     ]
    }
   ],
   "source": [
    "# TabNet\n",
    "!pip install --no-index --find-links /kaggle/input/pytorchtabnet/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n",
    "# Iterative Stratification\n",
    "!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023205,
     "end_time": "2021-04-16T08:44:42.746223",
     "exception": false,
     "start_time": "2021-04-16T08:44:42.723018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Loading Libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-04-16T08:44:42.801711Z",
     "iopub.status.busy": "2021-04-16T08:44:42.800898Z",
     "iopub.status.idle": "2021-04-16T08:44:45.158963Z",
     "shell.execute_reply": "2021-04-16T08:44:45.157835Z"
    },
    "papermill": {
     "duration": 2.389426,
     "end_time": "2021-04-16T08:44:45.159097",
     "exception": false,
     "start_time": "2021-04-16T08:44:42.769671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### General ###\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"../input/rank-gauss\")\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "\n",
    "### Data Wrangling ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from gauss_rank_scaler import GaussRankScaler\n",
    "\n",
    "### Data Visualization ###\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "### Machine Learning ###\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "### Deep Learning ###\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# Tabnet \n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "### Make prettier the prints ###\n",
    "from colorama import Fore\n",
    "c_ = Fore.CYAN\n",
    "m_ = Fore.MAGENTA\n",
    "r_ = Fore.RED\n",
    "b_ = Fore.BLUE\n",
    "y_ = Fore.YELLOW\n",
    "g_ = Fore.GREEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028231,
     "end_time": "2021-04-16T08:44:45.216009",
     "exception": false,
     "start_time": "2021-04-16T08:44:45.187778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Reproducibility</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:44:45.279623Z",
     "iopub.status.busy": "2021-04-16T08:44:45.278879Z",
     "iopub.status.idle": "2021-04-16T08:44:45.623224Z",
     "shell.execute_reply": "2021-04-16T08:44:45.622451Z"
    },
    "papermill": {
     "duration": 0.379023,
     "end_time": "2021-04-16T08:44:45.623351",
     "exception": false,
     "start_time": "2021-04-16T08:44:45.244328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023786,
     "end_time": "2021-04-16T08:44:45.671276",
     "exception": false,
     "start_time": "2021-04-16T08:44:45.647490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Configuration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:44:45.725000Z",
     "iopub.status.busy": "2021-04-16T08:44:45.723164Z",
     "iopub.status.idle": "2021-04-16T08:44:45.725620Z",
     "shell.execute_reply": "2021-04-16T08:44:45.726035Z"
    },
    "papermill": {
     "duration": 0.031116,
     "end_time": "2021-04-16T08:44:45.726153",
     "exception": false,
     "start_time": "2021-04-16T08:44:45.695037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "data_path = \".../input/univai/\"\n",
    "no_ctl = True\n",
    "scale = \"rankgauss\"\n",
    "variance_threshould = 0.5\n",
    "decompo = \"PCA\"\n",
    "ncompo = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025035,
     "end_time": "2021-04-16T08:44:45.776009",
     "exception": false,
     "start_time": "2021-04-16T08:44:45.750974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Loading the Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:44:45.832665Z",
     "iopub.status.busy": "2021-04-16T08:44:45.831956Z",
     "iopub.status.idle": "2021-04-16T08:44:45.835816Z",
     "shell.execute_reply": "2021-04-16T08:44:45.835391Z"
    },
    "papermill": {
     "duration": 0.034507,
     "end_time": "2021-04-16T08:44:45.835896",
     "exception": false,
     "start_time": "2021-04-16T08:44:45.801389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dummies(df,dum_cols,val = 0):\n",
    "    if val == 0:\n",
    "        return df\n",
    "    return pd.get_dummies(df, prefix=None, prefix_sep='_', dummy_na=False, columns=dum_cols)\n",
    "\n",
    "def normalize(df):\n",
    "    return (df - df.mean(0) )/df.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:44:45.900166Z",
     "iopub.status.busy": "2021-04-16T08:44:45.899597Z",
     "iopub.status.idle": "2021-04-16T08:44:48.458126Z",
     "shell.execute_reply": "2021-04-16T08:44:48.457144Z"
    },
    "papermill": {
     "duration": 2.597027,
     "end_time": "2021-04-16T08:44:48.458237",
     "exception": false,
     "start_time": "2021-04-16T08:44:45.861210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/univai/train_encoded.csv')\n",
    "test_df  = pd.read_csv('../input/univai/test_encoded.csv')\n",
    "\n",
    "train_features = train_df.drop(['risk_flag'],axis=1)\n",
    "targets = train_df['risk_flag']\n",
    "\n",
    "\n",
    "test_features = test_df.drop(['risk_flag'],axis=1)\n",
    "submission = pd.read_csv('../input/univai/Sample Prediction Dataset.csv')\n",
    "\n",
    "concat = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "create_dummy = 1                #0 if dont want dummies\n",
    "dum_cols = ['age', 'experience', 'married', 'house_ownership',\n",
    "       'car_ownership', 'profession', 'city', 'state', 'current_job_years',\n",
    "       'current_house_years']\n",
    "\n",
    "concat = get_dummies(concat,dum_cols,create_dummy)\n",
    "norm_concat = normalize(concat)\n",
    "\n",
    "train = norm_concat[:len(train_features)]\n",
    "test  = norm_concat[len(train_features):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024098,
     "end_time": "2021-04-16T08:44:48.506755",
     "exception": false,
     "start_time": "2021-04-16T08:44:48.482657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Distributions Before Rank Gauss and PCA</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0242,
     "end_time": "2021-04-16T08:44:48.554987",
     "exception": false,
     "start_time": "2021-04-16T08:44:48.530787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <font color = \"green\">Distributions of the Train Set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024117,
     "end_time": "2021-04-16T08:44:48.603231",
     "exception": false,
     "start_time": "2021-04-16T08:44:48.579114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Rank Gauss Process</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:44:48.658722Z",
     "iopub.status.busy": "2021-04-16T08:44:48.657517Z",
     "iopub.status.idle": "2021-04-16T08:44:51.138029Z",
     "shell.execute_reply": "2021-04-16T08:44:51.138598Z"
    },
    "papermill": {
     "duration": 2.51122,
     "end_time": "2021-04-16T08:44:51.138753",
     "exception": false,
     "start_time": "2021-04-16T08:44:48.627533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>age_21</th>\n",
       "      <th>age_22</th>\n",
       "      <th>age_23</th>\n",
       "      <th>age_24</th>\n",
       "      <th>age_25</th>\n",
       "      <th>age_26</th>\n",
       "      <th>age_27</th>\n",
       "      <th>age_28</th>\n",
       "      <th>age_29</th>\n",
       "      <th>...</th>\n",
       "      <th>current_job_years_10</th>\n",
       "      <th>current_job_years_11</th>\n",
       "      <th>current_job_years_12</th>\n",
       "      <th>current_job_years_13</th>\n",
       "      <th>current_job_years_14</th>\n",
       "      <th>current_house_years_10</th>\n",
       "      <th>current_house_years_11</th>\n",
       "      <th>current_house_years_12</th>\n",
       "      <th>current_house_years_13</th>\n",
       "      <th>current_house_years_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.284860</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>7.547901</td>\n",
       "      <td>-0.131745</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>-0.193823</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>-0.489485</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>-0.509218</td>\n",
       "      <td>1.995099</td>\n",
       "      <td>-0.491127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.894740</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>-0.132487</td>\n",
       "      <td>-0.131745</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>-0.193823</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>-0.489485</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>-0.509218</td>\n",
       "      <td>1.995099</td>\n",
       "      <td>-0.491127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.350556</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>-0.132487</td>\n",
       "      <td>-0.131745</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>-0.193823</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>2.042957</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>-0.509218</td>\n",
       "      <td>-0.501226</td>\n",
       "      <td>-0.491127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.436599</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>-0.132487</td>\n",
       "      <td>-0.131745</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>-0.193823</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>-0.489485</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>1.963787</td>\n",
       "      <td>-0.501226</td>\n",
       "      <td>-0.491127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.267123</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>-0.132487</td>\n",
       "      <td>-0.131745</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>-0.193823</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>-0.489485</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>-0.509218</td>\n",
       "      <td>-0.501226</td>\n",
       "      <td>2.036127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279995</th>\n",
       "      <td>1.722329</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>-0.132487</td>\n",
       "      <td>-0.131745</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>-0.193823</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>2.042957</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>-0.509218</td>\n",
       "      <td>-0.501226</td>\n",
       "      <td>-0.491127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279996</th>\n",
       "      <td>-0.723881</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>-0.132487</td>\n",
       "      <td>-0.131745</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>-0.193823</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>-0.489485</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>-0.509218</td>\n",
       "      <td>-0.501226</td>\n",
       "      <td>2.036127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279997</th>\n",
       "      <td>1.071278</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>-0.132487</td>\n",
       "      <td>7.590388</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>-0.193823</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>-0.489485</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>-0.509218</td>\n",
       "      <td>1.995099</td>\n",
       "      <td>-0.491127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279998</th>\n",
       "      <td>1.555036</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>-0.132487</td>\n",
       "      <td>-0.131745</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>5.159338</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>-0.489485</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>-0.509218</td>\n",
       "      <td>-0.501226</td>\n",
       "      <td>2.036127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279999</th>\n",
       "      <td>1.477236</td>\n",
       "      <td>-0.130193</td>\n",
       "      <td>-0.134248</td>\n",
       "      <td>-0.132487</td>\n",
       "      <td>-0.131745</td>\n",
       "      <td>-0.135306</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.125695</td>\n",
       "      <td>-0.128208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>-0.239086</td>\n",
       "      <td>-0.208974</td>\n",
       "      <td>-0.193823</td>\n",
       "      <td>-0.167845</td>\n",
       "      <td>2.042957</td>\n",
       "      <td>-0.508868</td>\n",
       "      <td>-0.509218</td>\n",
       "      <td>-0.501226</td>\n",
       "      <td>-0.491127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280000 rows × 519 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          income    age_21    age_22    age_23    age_24    age_25    age_26  \\\n",
       "0      -1.284860 -0.130193 -0.134248  7.547901 -0.131745 -0.135306 -0.129368   \n",
       "1       0.894740 -0.130193 -0.134248 -0.132487 -0.131745 -0.135306 -0.129368   \n",
       "2      -0.350556 -0.130193 -0.134248 -0.132487 -0.131745 -0.135306 -0.129368   \n",
       "3       0.436599 -0.130193 -0.134248 -0.132487 -0.131745 -0.135306 -0.129368   \n",
       "4       0.267123 -0.130193 -0.134248 -0.132487 -0.131745 -0.135306 -0.129368   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "279995  1.722329 -0.130193 -0.134248 -0.132487 -0.131745 -0.135306 -0.129368   \n",
       "279996 -0.723881 -0.130193 -0.134248 -0.132487 -0.131745 -0.135306 -0.129368   \n",
       "279997  1.071278 -0.130193 -0.134248 -0.132487  7.590388 -0.135306 -0.129368   \n",
       "279998  1.555036 -0.130193 -0.134248 -0.132487 -0.131745 -0.135306 -0.129368   \n",
       "279999  1.477236 -0.130193 -0.134248 -0.132487 -0.131745 -0.135306 -0.129368   \n",
       "\n",
       "          age_27    age_28    age_29  ...  current_job_years_10  \\\n",
       "0      -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "1      -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "2      -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "3      -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "4      -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "...          ...       ...       ...  ...                   ...   \n",
       "279995 -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "279996 -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "279997 -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "279998 -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "279999 -0.145464 -0.125695 -0.128208  ...             -0.258505   \n",
       "\n",
       "        current_job_years_11  current_job_years_12  current_job_years_13  \\\n",
       "0                  -0.239086             -0.208974             -0.193823   \n",
       "1                  -0.239086             -0.208974             -0.193823   \n",
       "2                  -0.239086             -0.208974             -0.193823   \n",
       "3                  -0.239086             -0.208974             -0.193823   \n",
       "4                  -0.239086             -0.208974             -0.193823   \n",
       "...                      ...                   ...                   ...   \n",
       "279995             -0.239086             -0.208974             -0.193823   \n",
       "279996             -0.239086             -0.208974             -0.193823   \n",
       "279997             -0.239086             -0.208974             -0.193823   \n",
       "279998             -0.239086             -0.208974              5.159338   \n",
       "279999             -0.239086             -0.208974             -0.193823   \n",
       "\n",
       "        current_job_years_14  current_house_years_10  current_house_years_11  \\\n",
       "0                  -0.167845               -0.489485               -0.508868   \n",
       "1                  -0.167845               -0.489485               -0.508868   \n",
       "2                  -0.167845                2.042957               -0.508868   \n",
       "3                  -0.167845               -0.489485               -0.508868   \n",
       "4                  -0.167845               -0.489485               -0.508868   \n",
       "...                      ...                     ...                     ...   \n",
       "279995             -0.167845                2.042957               -0.508868   \n",
       "279996             -0.167845               -0.489485               -0.508868   \n",
       "279997             -0.167845               -0.489485               -0.508868   \n",
       "279998             -0.167845               -0.489485               -0.508868   \n",
       "279999             -0.167845                2.042957               -0.508868   \n",
       "\n",
       "        current_house_years_12  current_house_years_13  current_house_years_14  \n",
       "0                    -0.509218                1.995099               -0.491127  \n",
       "1                    -0.509218                1.995099               -0.491127  \n",
       "2                    -0.509218               -0.501226               -0.491127  \n",
       "3                     1.963787               -0.501226               -0.491127  \n",
       "4                    -0.509218               -0.501226                2.036127  \n",
       "...                        ...                     ...                     ...  \n",
       "279995               -0.509218               -0.501226               -0.491127  \n",
       "279996               -0.509218               -0.501226                2.036127  \n",
       "279997               -0.509218                1.995099               -0.491127  \n",
       "279998               -0.509218               -0.501226                2.036127  \n",
       "279999               -0.509218               -0.501226               -0.491127  \n",
       "\n",
       "[280000 rows x 519 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all = pd.concat([train, test], ignore_index = True)\n",
    "cols_numeric = [feat for feat in list(data_all.columns)]\n",
    "mask = (data_all[cols_numeric].var() >= variance_threshould).values\n",
    "tmp = data_all[cols_numeric].loc[:, mask]\n",
    "data_all = pd.DataFrame(tmp,columns = cols_numeric)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:44:51.215656Z",
     "iopub.status.busy": "2021-04-16T08:44:51.213604Z",
     "iopub.status.idle": "2021-04-16T08:45:09.552160Z",
     "shell.execute_reply": "2021-04-16T08:45:09.551641Z"
    },
    "papermill": {
     "duration": 18.381124,
     "end_time": "2021-04-16T08:45:09.552291",
     "exception": false,
     "start_time": "2021-04-16T08:44:51.171167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m Rank Gauss\n"
     ]
    }
   ],
   "source": [
    "def scale_minmax(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "def scale_norm(col):\n",
    "    return (col - col.mean()) / col.std()\n",
    "\n",
    "if scale == \"boxcox\":\n",
    "    print(b_, \"boxcox\")\n",
    "    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis = 0)\n",
    "    trans = []\n",
    "    for feat in cols_numeric:\n",
    "        trans_var, lambda_var = stats.boxcox(data_all[feat].dropna() + 1)\n",
    "        trans.append(scale_minmax(trans_var))\n",
    "    data_all[cols_numeric] = np.asarray(trans).T\n",
    "    \n",
    "elif scale == \"norm\":\n",
    "    print(b_, \"norm\")\n",
    "    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_norm, axis = 0)\n",
    "    \n",
    "elif scale == \"minmax\":\n",
    "    print(b_, \"minmax\")\n",
    "    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis = 0)\n",
    "    \n",
    "elif scale == \"rankgauss\":\n",
    "    ### Rank Gauss ###\n",
    "    print(b_, \"Rank Gauss\")\n",
    "    scaler = GaussRankScaler()\n",
    "    data_all[cols_numeric] = scaler.fit_transform(data_all[cols_numeric])\n",
    "    \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025214,
     "end_time": "2021-04-16T08:45:09.603612",
     "exception": false,
     "start_time": "2021-04-16T08:45:09.578398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Principal Component Analysis</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:09.660919Z",
     "iopub.status.busy": "2021-04-16T08:45:09.660069Z",
     "iopub.status.idle": "2021-04-16T08:45:24.643877Z",
     "shell.execute_reply": "2021-04-16T08:45:24.642963Z"
    },
    "papermill": {
     "duration": 15.014887,
     "end_time": "2021-04-16T08:45:24.643986",
     "exception": false,
     "start_time": "2021-04-16T08:45:09.629099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m PCA\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "if decompo == \"PCA\":\n",
    "    print(b_, \"PCA\")\n",
    "    \n",
    "    pca_genes = PCA(n_components = ncompo,\n",
    "                    random_state = seed).fit_transform(data_all)\n",
    "    \n",
    "    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo)])\n",
    "    data_all = pd.concat((data_all, pca_genes), axis=1)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:24.712520Z",
     "iopub.status.busy": "2021-04-16T08:45:24.711283Z",
     "iopub.status.idle": "2021-04-16T08:45:24.778809Z",
     "shell.execute_reply": "2021-04-16T08:45:24.779238Z"
    },
    "papermill": {
     "duration": 0.109019,
     "end_time": "2021-04-16T08:45:24.779377",
     "exception": false,
     "start_time": "2021-04-16T08:45:24.670358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>age_21</th>\n",
       "      <th>age_22</th>\n",
       "      <th>age_23</th>\n",
       "      <th>age_24</th>\n",
       "      <th>age_25</th>\n",
       "      <th>age_26</th>\n",
       "      <th>age_27</th>\n",
       "      <th>age_28</th>\n",
       "      <th>age_29</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_g-70</th>\n",
       "      <th>pca_g-71</th>\n",
       "      <th>pca_g-72</th>\n",
       "      <th>pca_g-73</th>\n",
       "      <th>pca_g-74</th>\n",
       "      <th>pca_g-75</th>\n",
       "      <th>pca_g-76</th>\n",
       "      <th>pca_g-77</th>\n",
       "      <th>pca_g-78</th>\n",
       "      <th>pca_g-79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.803407</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233235</td>\n",
       "      <td>-0.229491</td>\n",
       "      <td>0.926818</td>\n",
       "      <td>1.075861</td>\n",
       "      <td>0.313440</td>\n",
       "      <td>0.471901</td>\n",
       "      <td>-0.203552</td>\n",
       "      <td>0.937039</td>\n",
       "      <td>1.077593</td>\n",
       "      <td>0.878844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.498481</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.429634</td>\n",
       "      <td>-0.730794</td>\n",
       "      <td>0.156615</td>\n",
       "      <td>-0.628841</td>\n",
       "      <td>-0.782229</td>\n",
       "      <td>-0.333093</td>\n",
       "      <td>0.116770</td>\n",
       "      <td>0.367585</td>\n",
       "      <td>-0.751161</td>\n",
       "      <td>1.512482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.178755</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170511</td>\n",
       "      <td>0.940855</td>\n",
       "      <td>0.354057</td>\n",
       "      <td>-1.175569</td>\n",
       "      <td>-0.845901</td>\n",
       "      <td>-0.837412</td>\n",
       "      <td>-1.298231</td>\n",
       "      <td>0.841301</td>\n",
       "      <td>0.652347</td>\n",
       "      <td>-0.315038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.228206</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.133942</td>\n",
       "      <td>0.218419</td>\n",
       "      <td>-0.290282</td>\n",
       "      <td>-0.020977</td>\n",
       "      <td>-1.377551</td>\n",
       "      <td>0.507345</td>\n",
       "      <td>-0.128129</td>\n",
       "      <td>0.179552</td>\n",
       "      <td>-0.587353</td>\n",
       "      <td>1.702142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138792</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.723791</td>\n",
       "      <td>-0.889688</td>\n",
       "      <td>0.663794</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.915554</td>\n",
       "      <td>1.626079</td>\n",
       "      <td>1.238566</td>\n",
       "      <td>-0.252754</td>\n",
       "      <td>-0.503776</td>\n",
       "      <td>0.977225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279995</th>\n",
       "      <td>1.878502</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785566</td>\n",
       "      <td>0.417115</td>\n",
       "      <td>-0.109288</td>\n",
       "      <td>-0.220912</td>\n",
       "      <td>1.122295</td>\n",
       "      <td>0.192071</td>\n",
       "      <td>-0.156663</td>\n",
       "      <td>-0.651637</td>\n",
       "      <td>-0.356651</td>\n",
       "      <td>0.401281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279996</th>\n",
       "      <td>-0.383637</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071508</td>\n",
       "      <td>-0.531957</td>\n",
       "      <td>0.559303</td>\n",
       "      <td>-0.618249</td>\n",
       "      <td>0.399950</td>\n",
       "      <td>0.598856</td>\n",
       "      <td>0.713734</td>\n",
       "      <td>-0.160126</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.981389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279997</th>\n",
       "      <td>0.620838</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306562</td>\n",
       "      <td>-0.777144</td>\n",
       "      <td>0.420654</td>\n",
       "      <td>0.155388</td>\n",
       "      <td>0.314216</td>\n",
       "      <td>-1.108431</td>\n",
       "      <td>0.924523</td>\n",
       "      <td>-0.300422</td>\n",
       "      <td>1.473142</td>\n",
       "      <td>-0.899735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279998</th>\n",
       "      <td>1.142869</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069656</td>\n",
       "      <td>0.884330</td>\n",
       "      <td>0.024342</td>\n",
       "      <td>0.121181</td>\n",
       "      <td>1.353447</td>\n",
       "      <td>0.518808</td>\n",
       "      <td>-0.472315</td>\n",
       "      <td>0.502064</td>\n",
       "      <td>-1.083616</td>\n",
       "      <td>0.665906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279999</th>\n",
       "      <td>1.012893</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>-2.751064</td>\n",
       "      <td>...</td>\n",
       "      <td>1.188237</td>\n",
       "      <td>-0.456054</td>\n",
       "      <td>-0.586164</td>\n",
       "      <td>0.171806</td>\n",
       "      <td>0.768936</td>\n",
       "      <td>0.113917</td>\n",
       "      <td>0.707345</td>\n",
       "      <td>-1.029360</td>\n",
       "      <td>0.327457</td>\n",
       "      <td>0.678060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280000 rows × 599 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          income    age_21    age_22    age_23    age_24    age_25    age_26  \\\n",
       "0      -0.803407 -2.751064 -2.751064  2.751064 -2.751064 -2.751064 -2.751064   \n",
       "1       0.498481 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064   \n",
       "2      -0.178755 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064   \n",
       "3       0.228206 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064   \n",
       "4       0.138792 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "279995  1.878502 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064   \n",
       "279996 -0.383637 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064   \n",
       "279997  0.620838 -2.751064 -2.751064 -2.751064  2.751064 -2.751064 -2.751064   \n",
       "279998  1.142869 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064   \n",
       "279999  1.012893 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064 -2.751064   \n",
       "\n",
       "          age_27    age_28    age_29  ...  pca_g-70  pca_g-71  pca_g-72  \\\n",
       "0      -2.751064 -2.751064 -2.751064  ... -0.233235 -0.229491  0.926818   \n",
       "1      -2.751064 -2.751064 -2.751064  ... -0.429634 -0.730794  0.156615   \n",
       "2      -2.751064 -2.751064 -2.751064  ...  0.170511  0.940855  0.354057   \n",
       "3      -2.751064 -2.751064 -2.751064  ... -1.133942  0.218419 -0.290282   \n",
       "4      -2.751064 -2.751064 -2.751064  ... -0.723791 -0.889688  0.663794   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "279995 -2.751064 -2.751064 -2.751064  ...  0.785566  0.417115 -0.109288   \n",
       "279996 -2.751064 -2.751064 -2.751064  ... -0.071508 -0.531957  0.559303   \n",
       "279997 -2.751064 -2.751064 -2.751064  ... -0.306562 -0.777144  0.420654   \n",
       "279998 -2.751064 -2.751064 -2.751064  ...  0.069656  0.884330  0.024342   \n",
       "279999 -2.751064 -2.751064 -2.751064  ...  1.188237 -0.456054 -0.586164   \n",
       "\n",
       "        pca_g-73  pca_g-74  pca_g-75  pca_g-76  pca_g-77  pca_g-78  pca_g-79  \n",
       "0       1.075861  0.313440  0.471901 -0.203552  0.937039  1.077593  0.878844  \n",
       "1      -0.628841 -0.782229 -0.333093  0.116770  0.367585 -0.751161  1.512482  \n",
       "2      -1.175569 -0.845901 -0.837412 -1.298231  0.841301  0.652347 -0.315038  \n",
       "3      -0.020977 -1.377551  0.507345 -0.128129  0.179552 -0.587353  1.702142  \n",
       "4       0.022248  0.915554  1.626079  1.238566 -0.252754 -0.503776  0.977225  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "279995 -0.220912  1.122295  0.192071 -0.156663 -0.651637 -0.356651  0.401281  \n",
       "279996 -0.618249  0.399950  0.598856  0.713734 -0.160126  0.003642  0.981389  \n",
       "279997  0.155388  0.314216 -1.108431  0.924523 -0.300422  1.473142 -0.899735  \n",
       "279998  0.121181  1.353447  0.518808 -0.472315  0.502064 -1.083616  0.665906  \n",
       "279999  0.171806  0.768936  0.113917  0.707345 -1.029360  0.327457  0.678060  \n",
       "\n",
       "[280000 rows x 599 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027179,
     "end_time": "2021-04-16T08:45:24.833596",
     "exception": false,
     "start_time": "2021-04-16T08:45:24.806417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">One Hot</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026377,
     "end_time": "2021-04-16T08:45:24.886569",
     "exception": false,
     "start_time": "2021-04-16T08:45:24.860192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can confirme that the shapes of data got close to the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:25.760719Z",
     "iopub.status.busy": "2021-04-16T08:45:24.952037Z",
     "iopub.status.idle": "2021-04-16T08:45:27.288671Z",
     "shell.execute_reply": "2021-04-16T08:45:27.290064Z"
    },
    "papermill": {
     "duration": 2.375414,
     "end_time": "2021-04-16T08:45:27.290295",
     "exception": false,
     "start_time": "2021-04-16T08:45:24.914881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data_all.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:27.652860Z",
     "iopub.status.busy": "2021-04-16T08:45:27.651740Z",
     "iopub.status.idle": "2021-04-16T08:45:29.566384Z",
     "shell.execute_reply": "2021-04-16T08:45:29.565125Z"
    },
    "papermill": {
     "duration": 2.138023,
     "end_time": "2021-04-16T08:45:29.566508",
     "exception": false,
     "start_time": "2021-04-16T08:45:27.428485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data_all.pickle\", \"rb\") as f:\n",
    "    data_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:29.629586Z",
     "iopub.status.busy": "2021-04-16T08:45:29.628902Z",
     "iopub.status.idle": "2021-04-16T08:45:29.632732Z",
     "shell.execute_reply": "2021-04-16T08:45:29.632292Z"
    },
    "papermill": {
     "duration": 0.037257,
     "end_time": "2021-04-16T08:45:29.632857",
     "exception": false,
     "start_time": "2021-04-16T08:45:29.595600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df and test_df\n",
    "\n",
    "train_df = data_all[: train.shape[0]]\n",
    "train_df.reset_index(drop = True, inplace = True)\n",
    "# The following line it's a bad practice in my opinion, targets on train set\n",
    "#train_df = pd.concat([train_df, targets], axis = 1)\n",
    "test_df = data_all[train_df.shape[0]: ]\n",
    "test_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:29.700271Z",
     "iopub.status.busy": "2021-04-16T08:45:29.698614Z",
     "iopub.status.idle": "2021-04-16T08:45:29.702336Z",
     "shell.execute_reply": "2021-04-16T08:45:29.701839Z"
    },
    "papermill": {
     "duration": 0.039117,
     "end_time": "2021-04-16T08:45:29.702424",
     "exception": false,
     "start_time": "2021-04-16T08:45:29.663307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mtrain_df.shape: \u001b[31m(252000, 599)\n",
      "\u001b[34mtest_df.shape: \u001b[31m(28000, 599)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{b_}train_df.shape: {r_}{train_df.shape}\")\n",
    "print(f\"{b_}test_df.shape: {r_}{test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:32.586061Z",
     "iopub.status.busy": "2021-04-16T08:45:32.584756Z",
     "iopub.status.idle": "2021-04-16T08:45:32.667585Z",
     "shell.execute_reply": "2021-04-16T08:45:32.668006Z"
    },
    "papermill": {
     "duration": 2.937505,
     "end_time": "2021-04-16T08:45:32.668122",
     "exception": false,
     "start_time": "2021-04-16T08:45:29.730617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mX_test.shape: \u001b[31m(28000, 599)\n"
     ]
    }
   ],
   "source": [
    "X_test = test_df.values\n",
    "print(f\"{b_}X_test.shape: {r_}{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028595,
     "end_time": "2021-04-16T08:45:32.724628",
     "exception": false,
     "start_time": "2021-04-16T08:45:32.696033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font color = \"seagreen\">Modeling</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029085,
     "end_time": "2021-04-16T08:45:32.783586",
     "exception": false,
     "start_time": "2021-04-16T08:45:32.754501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Model Parameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:32.847208Z",
     "iopub.status.busy": "2021-04-16T08:45:32.846667Z",
     "iopub.status.idle": "2021-04-16T08:45:32.850576Z",
     "shell.execute_reply": "2021-04-16T08:45:32.849965Z"
    },
    "papermill": {
     "duration": 0.038542,
     "end_time": "2021-04-16T08:45:32.850700",
     "exception": false,
     "start_time": "2021-04-16T08:45:32.812158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH = 150\n",
    "# n_d and n_a are different from the original work, 32 instead of 24\n",
    "# This is the first change in the code from the original\n",
    "tabnet_params = dict(\n",
    "    n_d = 32,\n",
    "    n_a = 32,\n",
    "    n_steps = 1,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    optimizer_fn = optim.Adam,\n",
    "    optimizer_params = dict(lr = 2e-3, weight_decay = 1e-5),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"max\", patience = 3, min_lr = 1e-6, factor = 0.33, verbose = True),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = seed,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02836,
     "end_time": "2021-04-16T08:45:32.906925",
     "exception": false,
     "start_time": "2021-04-16T08:45:32.878565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <font color = \"green\">Custom Metric</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:32.972543Z",
     "iopub.status.busy": "2021-04-16T08:45:32.970775Z",
     "iopub.status.idle": "2021-04-16T08:45:32.973123Z",
     "shell.execute_reply": "2021-04-16T08:45:32.973541Z"
    },
    "papermill": {
     "duration": 0.038434,
     "end_time": "2021-04-16T08:45:32.973642",
     "exception": false,
     "start_time": "2021-04-16T08:45:32.935208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogitsLogLoss(Metric):\n",
    "    \"\"\"\n",
    "    LogLoss with sigmoid applied\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute LogLoss of predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: np.ndarray\n",
    "            Target matrix or vector\n",
    "        y_score: np.ndarray\n",
    "            Score matrix or vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            float\n",
    "            LogLoss of predictions vs targets.\n",
    "        \"\"\"\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:33.036326Z",
     "iopub.status.busy": "2021-04-16T08:45:33.034980Z",
     "iopub.status.idle": "2021-04-16T08:45:33.036978Z",
     "shell.execute_reply": "2021-04-16T08:45:33.037888Z"
    },
    "papermill": {
     "duration": 0.0361,
     "end_time": "2021-04-16T08:45:33.037989",
     "exception": false,
     "start_time": "2021-04-16T08:45:33.001889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class roc_auc_c(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"roc_auc_c\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return roc_auc_score(y_true, np.array(y_score).round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028187,
     "end_time": "2021-04-16T08:45:33.093996",
     "exception": false,
     "start_time": "2021-04-16T08:45:33.065809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font color = \"seagreen\">Training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T08:45:33.165321Z",
     "iopub.status.busy": "2021-04-16T08:45:33.162497Z",
     "iopub.status.idle": "2021-04-16T13:16:10.059295Z",
     "shell.execute_reply": "2021-04-16T13:16:10.058507Z"
    },
    "papermill": {
     "duration": 16236.937491,
     "end_time": "2021-04-16T13:16:10.059433",
     "exception": false,
     "start_time": "2021-04-16T08:45:33.121942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m FOLDS:  \u001b[31m 1\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40267 | val_roc_auc_c: 0.53772 |  0:00:20s\n",
      "epoch 1  | loss: 0.36035 | val_roc_auc_c: 0.69718 |  0:00:40s\n",
      "epoch 2  | loss: 0.33749 | val_roc_auc_c: 0.80516 |  0:01:00s\n",
      "epoch 3  | loss: 0.30817 | val_roc_auc_c: 0.85604 |  0:01:20s\n",
      "epoch 4  | loss: 0.28203 | val_roc_auc_c: 0.87984 |  0:01:40s\n",
      "epoch 5  | loss: 0.2629  | val_roc_auc_c: 0.88736 |  0:02:00s\n",
      "epoch 6  | loss: 0.25155 | val_roc_auc_c: 0.8923  |  0:02:20s\n",
      "epoch 7  | loss: 0.24172 | val_roc_auc_c: 0.89486 |  0:02:40s\n",
      "epoch 8  | loss: 0.23432 | val_roc_auc_c: 0.89863 |  0:03:01s\n",
      "epoch 9  | loss: 0.22975 | val_roc_auc_c: 0.89959 |  0:03:22s\n",
      "epoch 10 | loss: 0.22446 | val_roc_auc_c: 0.89968 |  0:03:42s\n",
      "epoch 11 | loss: 0.21942 | val_roc_auc_c: 0.9009  |  0:04:03s\n",
      "epoch 12 | loss: 0.21476 | val_roc_auc_c: 0.90426 |  0:04:24s\n",
      "epoch 13 | loss: 0.212   | val_roc_auc_c: 0.90566 |  0:04:44s\n",
      "epoch 14 | loss: 0.20844 | val_roc_auc_c: 0.90455 |  0:05:05s\n",
      "epoch 15 | loss: 0.20569 | val_roc_auc_c: 0.90604 |  0:05:26s\n",
      "epoch 16 | loss: 0.20215 | val_roc_auc_c: 0.90317 |  0:05:46s\n",
      "epoch 17 | loss: 0.19869 | val_roc_auc_c: 0.9046  |  0:06:07s\n",
      "epoch 18 | loss: 0.19739 | val_roc_auc_c: 0.90623 |  0:06:28s\n",
      "epoch 19 | loss: 0.19373 | val_roc_auc_c: 0.90786 |  0:06:48s\n",
      "epoch 20 | loss: 0.19123 | val_roc_auc_c: 0.90649 |  0:07:09s\n",
      "epoch 21 | loss: 0.18993 | val_roc_auc_c: 0.90813 |  0:07:30s\n",
      "epoch 22 | loss: 0.18776 | val_roc_auc_c: 0.90543 |  0:07:50s\n",
      "epoch 23 | loss: 0.18643 | val_roc_auc_c: 0.90875 |  0:08:11s\n",
      "epoch 24 | loss: 0.18465 | val_roc_auc_c: 0.90904 |  0:08:32s\n",
      "epoch 25 | loss: 0.18186 | val_roc_auc_c: 0.9095  |  0:08:53s\n",
      "epoch 26 | loss: 0.1801  | val_roc_auc_c: 0.91094 |  0:09:14s\n",
      "epoch 27 | loss: 0.17879 | val_roc_auc_c: 0.90989 |  0:09:34s\n",
      "epoch 28 | loss: 0.17787 | val_roc_auc_c: 0.91088 |  0:09:56s\n",
      "epoch 29 | loss: 0.17649 | val_roc_auc_c: 0.91072 |  0:10:17s\n",
      "epoch 30 | loss: 0.1753  | val_roc_auc_c: 0.90868 |  0:10:37s\n",
      "Epoch    31: reducing learning rate of group 0 to 6.6000e-04.\n",
      "epoch 31 | loss: 0.17019 | val_roc_auc_c: 0.91214 |  0:10:58s\n",
      "epoch 32 | loss: 0.16651 | val_roc_auc_c: 0.9134  |  0:11:19s\n",
      "epoch 33 | loss: 0.16425 | val_roc_auc_c: 0.91413 |  0:11:40s\n",
      "epoch 34 | loss: 0.16383 | val_roc_auc_c: 0.91414 |  0:12:01s\n",
      "epoch 35 | loss: 0.16138 | val_roc_auc_c: 0.9134  |  0:12:22s\n",
      "epoch 36 | loss: 0.16087 | val_roc_auc_c: 0.91375 |  0:12:43s\n",
      "epoch 37 | loss: 0.16033 | val_roc_auc_c: 0.91335 |  0:13:04s\n",
      "Epoch    38: reducing learning rate of group 0 to 2.1780e-04.\n",
      "epoch 38 | loss: 0.15958 | val_roc_auc_c: 0.91352 |  0:13:25s\n",
      "epoch 39 | loss: 0.15812 | val_roc_auc_c: 0.91412 |  0:13:46s\n",
      "epoch 40 | loss: 0.15781 | val_roc_auc_c: 0.91448 |  0:14:07s\n",
      "epoch 41 | loss: 0.15688 | val_roc_auc_c: 0.91463 |  0:14:28s\n",
      "epoch 42 | loss: 0.1568  | val_roc_auc_c: 0.9142  |  0:14:49s\n",
      "epoch 43 | loss: 0.15608 | val_roc_auc_c: 0.91468 |  0:15:10s\n",
      "epoch 44 | loss: 0.15605 | val_roc_auc_c: 0.91469 |  0:15:32s\n",
      "epoch 45 | loss: 0.15555 | val_roc_auc_c: 0.9146  |  0:15:52s\n",
      "Epoch    46: reducing learning rate of group 0 to 7.1874e-05.\n",
      "epoch 46 | loss: 0.15579 | val_roc_auc_c: 0.91539 |  0:16:13s\n",
      "epoch 47 | loss: 0.15547 | val_roc_auc_c: 0.9154  |  0:16:34s\n",
      "epoch 48 | loss: 0.15478 | val_roc_auc_c: 0.91525 |  0:16:56s\n",
      "epoch 49 | loss: 0.15489 | val_roc_auc_c: 0.91523 |  0:17:17s\n",
      "epoch 50 | loss: 0.15488 | val_roc_auc_c: 0.91533 |  0:17:38s\n",
      "Epoch    51: reducing learning rate of group 0 to 2.3718e-05.\n",
      "epoch 51 | loss: 0.15495 | val_roc_auc_c: 0.91546 |  0:17:59s\n",
      "epoch 52 | loss: 0.15434 | val_roc_auc_c: 0.91545 |  0:18:20s\n",
      "epoch 53 | loss: 0.15498 | val_roc_auc_c: 0.91549 |  0:18:41s\n",
      "epoch 54 | loss: 0.15473 | val_roc_auc_c: 0.91565 |  0:19:02s\n",
      "epoch 55 | loss: 0.15412 | val_roc_auc_c: 0.91528 |  0:19:24s\n",
      "epoch 56 | loss: 0.15373 | val_roc_auc_c: 0.9153  |  0:19:45s\n",
      "epoch 57 | loss: 0.15445 | val_roc_auc_c: 0.91562 |  0:20:06s\n",
      "epoch 58 | loss: 0.15422 | val_roc_auc_c: 0.9157  |  0:20:27s\n",
      "Epoch    59: reducing learning rate of group 0 to 7.8271e-06.\n",
      "epoch 59 | loss: 0.15468 | val_roc_auc_c: 0.91553 |  0:20:48s\n",
      "epoch 60 | loss: 0.15454 | val_roc_auc_c: 0.91567 |  0:21:09s\n",
      "epoch 61 | loss: 0.15371 | val_roc_auc_c: 0.91517 |  0:21:31s\n",
      "epoch 62 | loss: 0.15365 | val_roc_auc_c: 0.91511 |  0:21:52s\n",
      "Epoch    63: reducing learning rate of group 0 to 2.5829e-06.\n",
      "epoch 63 | loss: 0.15349 | val_roc_auc_c: 0.91571 |  0:22:13s\n",
      "epoch 64 | loss: 0.15426 | val_roc_auc_c: 0.91575 |  0:22:34s\n",
      "epoch 65 | loss: 0.1538  | val_roc_auc_c: 0.91551 |  0:22:55s\n",
      "epoch 66 | loss: 0.1537  | val_roc_auc_c: 0.91548 |  0:23:16s\n",
      "epoch 67 | loss: 0.15435 | val_roc_auc_c: 0.91515 |  0:23:37s\n",
      "epoch 68 | loss: 0.15415 | val_roc_auc_c: 0.91552 |  0:23:58s\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch 69 | loss: 0.15365 | val_roc_auc_c: 0.9156  |  0:24:19s\n",
      "epoch 70 | loss: 0.15418 | val_roc_auc_c: 0.91556 |  0:24:40s\n",
      "epoch 71 | loss: 0.15376 | val_roc_auc_c: 0.91559 |  0:25:01s\n",
      "epoch 72 | loss: 0.15307 | val_roc_auc_c: 0.91553 |  0:25:23s\n",
      "epoch 73 | loss: 0.15377 | val_roc_auc_c: 0.91548 |  0:25:44s\n",
      "epoch 74 | loss: 0.15478 | val_roc_auc_c: 0.91583 |  0:26:05s\n",
      "epoch 75 | loss: 0.15476 | val_roc_auc_c: 0.91556 |  0:26:26s\n",
      "epoch 76 | loss: 0.15384 | val_roc_auc_c: 0.91549 |  0:26:47s\n",
      "epoch 77 | loss: 0.15396 | val_roc_auc_c: 0.91604 |  0:27:09s\n",
      "epoch 78 | loss: 0.15397 | val_roc_auc_c: 0.91553 |  0:27:30s\n",
      "epoch 79 | loss: 0.1536  | val_roc_auc_c: 0.91567 |  0:27:51s\n",
      "epoch 80 | loss: 0.15393 | val_roc_auc_c: 0.91551 |  0:28:12s\n",
      "epoch 81 | loss: 0.15395 | val_roc_auc_c: 0.91564 |  0:28:33s\n",
      "epoch 82 | loss: 0.15437 | val_roc_auc_c: 0.91553 |  0:28:54s\n",
      "epoch 83 | loss: 0.15421 | val_roc_auc_c: 0.91571 |  0:29:16s\n",
      "epoch 84 | loss: 0.15374 | val_roc_auc_c: 0.91533 |  0:29:37s\n",
      "epoch 85 | loss: 0.15385 | val_roc_auc_c: 0.91533 |  0:29:58s\n",
      "epoch 86 | loss: 0.15339 | val_roc_auc_c: 0.9155  |  0:30:19s\n",
      "epoch 87 | loss: 0.1538  | val_roc_auc_c: 0.91558 |  0:30:41s\n",
      "epoch 88 | loss: 0.15365 | val_roc_auc_c: 0.91552 |  0:31:02s\n",
      "epoch 89 | loss: 0.15396 | val_roc_auc_c: 0.91505 |  0:31:23s\n",
      "epoch 90 | loss: 0.15494 | val_roc_auc_c: 0.91525 |  0:31:44s\n",
      "epoch 91 | loss: 0.15439 | val_roc_auc_c: 0.91549 |  0:32:05s\n",
      "epoch 92 | loss: 0.15403 | val_roc_auc_c: 0.91561 |  0:32:27s\n",
      "epoch 93 | loss: 0.15375 | val_roc_auc_c: 0.91564 |  0:32:48s\n",
      "epoch 94 | loss: 0.15403 | val_roc_auc_c: 0.91577 |  0:33:09s\n",
      "epoch 95 | loss: 0.15458 | val_roc_auc_c: 0.91558 |  0:33:31s\n",
      "epoch 96 | loss: 0.15402 | val_roc_auc_c: 0.91557 |  0:33:52s\n",
      "epoch 97 | loss: 0.15462 | val_roc_auc_c: 0.91519 |  0:34:13s\n",
      "\n",
      "Early stopping occured at epoch 97 with best_epoch = 77 and best_val_roc_auc_c = 0.91604\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "\u001b[34m FOLDS:  \u001b[31m 2\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40305 | val_roc_auc_c: 0.53805 |  0:00:21s\n",
      "epoch 1  | loss: 0.35893 | val_roc_auc_c: 0.72802 |  0:00:42s\n",
      "epoch 2  | loss: 0.33699 | val_roc_auc_c: 0.80776 |  0:01:03s\n",
      "epoch 3  | loss: 0.30779 | val_roc_auc_c: 0.85748 |  0:01:24s\n",
      "epoch 4  | loss: 0.28182 | val_roc_auc_c: 0.88114 |  0:01:46s\n",
      "epoch 5  | loss: 0.2619  | val_roc_auc_c: 0.88581 |  0:02:08s\n",
      "epoch 6  | loss: 0.25055 | val_roc_auc_c: 0.88969 |  0:02:29s\n",
      "epoch 7  | loss: 0.24091 | val_roc_auc_c: 0.89341 |  0:02:51s\n",
      "epoch 8  | loss: 0.23336 | val_roc_auc_c: 0.89524 |  0:03:12s\n",
      "epoch 9  | loss: 0.22798 | val_roc_auc_c: 0.89748 |  0:03:33s\n",
      "epoch 10 | loss: 0.22359 | val_roc_auc_c: 0.90098 |  0:03:55s\n",
      "epoch 11 | loss: 0.21821 | val_roc_auc_c: 0.90104 |  0:04:16s\n",
      "epoch 12 | loss: 0.21337 | val_roc_auc_c: 0.9016  |  0:04:37s\n",
      "epoch 13 | loss: 0.2109  | val_roc_auc_c: 0.90305 |  0:04:59s\n",
      "epoch 14 | loss: 0.20742 | val_roc_auc_c: 0.90406 |  0:05:20s\n",
      "epoch 15 | loss: 0.2043  | val_roc_auc_c: 0.90372 |  0:05:42s\n",
      "epoch 16 | loss: 0.20081 | val_roc_auc_c: 0.90514 |  0:06:03s\n",
      "epoch 17 | loss: 0.19814 | val_roc_auc_c: 0.90496 |  0:06:25s\n",
      "epoch 18 | loss: 0.19567 | val_roc_auc_c: 0.90662 |  0:06:46s\n",
      "epoch 19 | loss: 0.19359 | val_roc_auc_c: 0.90641 |  0:07:08s\n",
      "epoch 20 | loss: 0.19124 | val_roc_auc_c: 0.90541 |  0:07:30s\n",
      "epoch 21 | loss: 0.18948 | val_roc_auc_c: 0.90858 |  0:07:51s\n",
      "epoch 22 | loss: 0.18741 | val_roc_auc_c: 0.90676 |  0:08:12s\n",
      "epoch 23 | loss: 0.18492 | val_roc_auc_c: 0.90885 |  0:08:34s\n",
      "epoch 24 | loss: 0.1836  | val_roc_auc_c: 0.91044 |  0:08:55s\n",
      "epoch 25 | loss: 0.18186 | val_roc_auc_c: 0.90882 |  0:09:17s\n",
      "epoch 26 | loss: 0.17959 | val_roc_auc_c: 0.91041 |  0:09:39s\n",
      "epoch 27 | loss: 0.17855 | val_roc_auc_c: 0.90835 |  0:10:00s\n",
      "epoch 28 | loss: 0.1778  | val_roc_auc_c: 0.90831 |  0:10:21s\n",
      "Epoch    29: reducing learning rate of group 0 to 6.6000e-04.\n",
      "epoch 29 | loss: 0.17094 | val_roc_auc_c: 0.91248 |  0:10:42s\n",
      "epoch 30 | loss: 0.16818 | val_roc_auc_c: 0.9132  |  0:11:03s\n",
      "epoch 31 | loss: 0.16573 | val_roc_auc_c: 0.91284 |  0:11:23s\n",
      "epoch 32 | loss: 0.16558 | val_roc_auc_c: 0.91236 |  0:11:44s\n",
      "epoch 33 | loss: 0.16312 | val_roc_auc_c: 0.91417 |  0:12:05s\n",
      "epoch 34 | loss: 0.16305 | val_roc_auc_c: 0.91369 |  0:12:25s\n",
      "epoch 35 | loss: 0.16309 | val_roc_auc_c: 0.91504 |  0:12:46s\n",
      "epoch 36 | loss: 0.16266 | val_roc_auc_c: 0.91488 |  0:13:07s\n",
      "epoch 37 | loss: 0.16096 | val_roc_auc_c: 0.91449 |  0:13:28s\n",
      "epoch 38 | loss: 0.16124 | val_roc_auc_c: 0.91399 |  0:13:50s\n",
      "epoch 39 | loss: 0.16079 | val_roc_auc_c: 0.91346 |  0:14:10s\n",
      "Epoch    40: reducing learning rate of group 0 to 2.1780e-04.\n",
      "epoch 40 | loss: 0.1598  | val_roc_auc_c: 0.91448 |  0:14:32s\n",
      "epoch 41 | loss: 0.1576  | val_roc_auc_c: 0.9146  |  0:14:53s\n",
      "epoch 42 | loss: 0.15675 | val_roc_auc_c: 0.91529 |  0:15:14s\n",
      "epoch 43 | loss: 0.15743 | val_roc_auc_c: 0.91545 |  0:15:35s\n",
      "epoch 44 | loss: 0.15692 | val_roc_auc_c: 0.91495 |  0:15:56s\n",
      "epoch 45 | loss: 0.15623 | val_roc_auc_c: 0.91555 |  0:16:16s\n",
      "epoch 46 | loss: 0.15583 | val_roc_auc_c: 0.91481 |  0:16:37s\n",
      "epoch 47 | loss: 0.15543 | val_roc_auc_c: 0.91525 |  0:16:57s\n",
      "epoch 48 | loss: 0.15547 | val_roc_auc_c: 0.91543 |  0:17:18s\n",
      "epoch 49 | loss: 0.15433 | val_roc_auc_c: 0.91478 |  0:17:38s\n",
      "Epoch    50: reducing learning rate of group 0 to 7.1874e-05.\n",
      "epoch 50 | loss: 0.15472 | val_roc_auc_c: 0.91548 |  0:17:59s\n",
      "epoch 51 | loss: 0.15492 | val_roc_auc_c: 0.91498 |  0:18:20s\n",
      "epoch 52 | loss: 0.15387 | val_roc_auc_c: 0.91539 |  0:18:41s\n",
      "epoch 53 | loss: 0.15455 | val_roc_auc_c: 0.91559 |  0:19:01s\n",
      "Epoch    54: reducing learning rate of group 0 to 2.3718e-05.\n",
      "epoch 54 | loss: 0.15395 | val_roc_auc_c: 0.91515 |  0:19:21s\n",
      "epoch 55 | loss: 0.15425 | val_roc_auc_c: 0.91536 |  0:19:42s\n",
      "epoch 56 | loss: 0.15348 | val_roc_auc_c: 0.91569 |  0:20:02s\n",
      "epoch 57 | loss: 0.15373 | val_roc_auc_c: 0.91546 |  0:20:23s\n",
      "epoch 58 | loss: 0.1531  | val_roc_auc_c: 0.91523 |  0:20:44s\n",
      "epoch 59 | loss: 0.15319 | val_roc_auc_c: 0.91564 |  0:21:04s\n",
      "epoch 60 | loss: 0.15332 | val_roc_auc_c: 0.91548 |  0:21:25s\n",
      "Epoch    61: reducing learning rate of group 0 to 7.8271e-06.\n",
      "epoch 61 | loss: 0.15342 | val_roc_auc_c: 0.91495 |  0:21:45s\n",
      "epoch 62 | loss: 0.15315 | val_roc_auc_c: 0.91536 |  0:22:06s\n",
      "epoch 63 | loss: 0.1532  | val_roc_auc_c: 0.91556 |  0:22:27s\n",
      "epoch 64 | loss: 0.1534  | val_roc_auc_c: 0.91513 |  0:22:47s\n",
      "Epoch    65: reducing learning rate of group 0 to 2.5829e-06.\n",
      "epoch 65 | loss: 0.15394 | val_roc_auc_c: 0.91546 |  0:23:08s\n",
      "epoch 66 | loss: 0.1532  | val_roc_auc_c: 0.91573 |  0:23:29s\n",
      "epoch 67 | loss: 0.1533  | val_roc_auc_c: 0.91522 |  0:23:49s\n",
      "epoch 68 | loss: 0.1532  | val_roc_auc_c: 0.91532 |  0:24:10s\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch 69 | loss: 0.15402 | val_roc_auc_c: 0.91539 |  0:24:31s\n",
      "epoch 70 | loss: 0.15278 | val_roc_auc_c: 0.91546 |  0:24:51s\n",
      "epoch 71 | loss: 0.15349 | val_roc_auc_c: 0.91549 |  0:25:12s\n",
      "epoch 72 | loss: 0.15354 | val_roc_auc_c: 0.91538 |  0:25:33s\n",
      "epoch 73 | loss: 0.15302 | val_roc_auc_c: 0.91565 |  0:25:54s\n",
      "epoch 74 | loss: 0.15254 | val_roc_auc_c: 0.91542 |  0:26:14s\n",
      "epoch 75 | loss: 0.15383 | val_roc_auc_c: 0.91571 |  0:26:35s\n",
      "epoch 76 | loss: 0.15328 | val_roc_auc_c: 0.91552 |  0:26:56s\n",
      "epoch 77 | loss: 0.15251 | val_roc_auc_c: 0.91541 |  0:27:16s\n",
      "epoch 78 | loss: 0.15321 | val_roc_auc_c: 0.91501 |  0:27:37s\n",
      "epoch 79 | loss: 0.1536  | val_roc_auc_c: 0.91583 |  0:27:57s\n",
      "epoch 80 | loss: 0.15344 | val_roc_auc_c: 0.91537 |  0:28:18s\n",
      "epoch 81 | loss: 0.15363 | val_roc_auc_c: 0.91512 |  0:28:38s\n",
      "epoch 82 | loss: 0.15292 | val_roc_auc_c: 0.91521 |  0:28:59s\n",
      "epoch 83 | loss: 0.15378 | val_roc_auc_c: 0.91511 |  0:29:20s\n",
      "epoch 84 | loss: 0.15345 | val_roc_auc_c: 0.91523 |  0:29:40s\n",
      "epoch 85 | loss: 0.15321 | val_roc_auc_c: 0.91531 |  0:30:01s\n",
      "epoch 86 | loss: 0.15354 | val_roc_auc_c: 0.91558 |  0:30:21s\n",
      "epoch 87 | loss: 0.15343 | val_roc_auc_c: 0.91553 |  0:30:42s\n",
      "epoch 88 | loss: 0.15318 | val_roc_auc_c: 0.9154  |  0:31:03s\n",
      "epoch 89 | loss: 0.15393 | val_roc_auc_c: 0.91589 |  0:31:23s\n",
      "epoch 90 | loss: 0.15268 | val_roc_auc_c: 0.91528 |  0:31:43s\n",
      "epoch 91 | loss: 0.153   | val_roc_auc_c: 0.91568 |  0:32:04s\n",
      "epoch 92 | loss: 0.15372 | val_roc_auc_c: 0.91531 |  0:32:25s\n",
      "epoch 93 | loss: 0.15297 | val_roc_auc_c: 0.91496 |  0:32:45s\n",
      "epoch 94 | loss: 0.15319 | val_roc_auc_c: 0.9157  |  0:33:06s\n",
      "epoch 95 | loss: 0.15335 | val_roc_auc_c: 0.91552 |  0:33:26s\n",
      "epoch 96 | loss: 0.15326 | val_roc_auc_c: 0.91572 |  0:33:47s\n",
      "epoch 97 | loss: 0.1538  | val_roc_auc_c: 0.91586 |  0:34:07s\n",
      "epoch 98 | loss: 0.15361 | val_roc_auc_c: 0.91508 |  0:34:28s\n",
      "epoch 99 | loss: 0.15298 | val_roc_auc_c: 0.91589 |  0:34:49s\n",
      "epoch 100| loss: 0.15334 | val_roc_auc_c: 0.91557 |  0:35:09s\n",
      "epoch 101| loss: 0.15346 | val_roc_auc_c: 0.91538 |  0:35:30s\n",
      "epoch 102| loss: 0.1538  | val_roc_auc_c: 0.91568 |  0:35:50s\n",
      "epoch 103| loss: 0.1532  | val_roc_auc_c: 0.91554 |  0:36:11s\n",
      "epoch 104| loss: 0.15366 | val_roc_auc_c: 0.91532 |  0:36:32s\n",
      "epoch 105| loss: 0.15303 | val_roc_auc_c: 0.91545 |  0:36:52s\n",
      "epoch 106| loss: 0.15356 | val_roc_auc_c: 0.91548 |  0:37:13s\n",
      "epoch 107| loss: 0.15312 | val_roc_auc_c: 0.91523 |  0:37:34s\n",
      "epoch 108| loss: 0.15295 | val_roc_auc_c: 0.91536 |  0:37:54s\n",
      "epoch 109| loss: 0.15316 | val_roc_auc_c: 0.91529 |  0:38:14s\n",
      "epoch 110| loss: 0.15357 | val_roc_auc_c: 0.91587 |  0:38:35s\n",
      "epoch 111| loss: 0.15344 | val_roc_auc_c: 0.9158  |  0:38:55s\n",
      "epoch 112| loss: 0.15361 | val_roc_auc_c: 0.9156  |  0:39:16s\n",
      "epoch 113| loss: 0.15328 | val_roc_auc_c: 0.91542 |  0:39:36s\n",
      "epoch 114| loss: 0.15339 | val_roc_auc_c: 0.91593 |  0:39:57s\n",
      "epoch 115| loss: 0.15325 | val_roc_auc_c: 0.91515 |  0:40:17s\n",
      "epoch 116| loss: 0.15265 | val_roc_auc_c: 0.91535 |  0:40:38s\n",
      "epoch 117| loss: 0.15333 | val_roc_auc_c: 0.91544 |  0:40:58s\n",
      "epoch 118| loss: 0.15282 | val_roc_auc_c: 0.9152  |  0:41:19s\n",
      "epoch 119| loss: 0.15269 | val_roc_auc_c: 0.91517 |  0:41:40s\n",
      "epoch 120| loss: 0.15305 | val_roc_auc_c: 0.91573 |  0:42:00s\n",
      "epoch 121| loss: 0.15411 | val_roc_auc_c: 0.9158  |  0:42:21s\n",
      "epoch 122| loss: 0.15322 | val_roc_auc_c: 0.91576 |  0:42:41s\n",
      "epoch 123| loss: 0.15332 | val_roc_auc_c: 0.91549 |  0:43:01s\n",
      "epoch 124| loss: 0.1529  | val_roc_auc_c: 0.91551 |  0:43:22s\n",
      "epoch 125| loss: 0.15353 | val_roc_auc_c: 0.9152  |  0:43:42s\n",
      "epoch 126| loss: 0.15366 | val_roc_auc_c: 0.91529 |  0:44:03s\n",
      "epoch 127| loss: 0.15301 | val_roc_auc_c: 0.91534 |  0:44:24s\n",
      "epoch 128| loss: 0.15291 | val_roc_auc_c: 0.91527 |  0:44:44s\n",
      "epoch 129| loss: 0.15382 | val_roc_auc_c: 0.91525 |  0:45:05s\n",
      "epoch 130| loss: 0.15309 | val_roc_auc_c: 0.91505 |  0:45:25s\n",
      "epoch 131| loss: 0.15241 | val_roc_auc_c: 0.91509 |  0:45:46s\n",
      "epoch 132| loss: 0.15288 | val_roc_auc_c: 0.91532 |  0:46:06s\n",
      "epoch 133| loss: 0.15308 | val_roc_auc_c: 0.91519 |  0:46:27s\n",
      "epoch 134| loss: 0.15256 | val_roc_auc_c: 0.91518 |  0:46:47s\n",
      "\n",
      "Early stopping occured at epoch 134 with best_epoch = 114 and best_val_roc_auc_c = 0.91593\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "\u001b[34m FOLDS:  \u001b[31m 3\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40247 | val_roc_auc_c: 0.52741 |  0:00:20s\n",
      "epoch 1  | loss: 0.35905 | val_roc_auc_c: 0.69519 |  0:00:41s\n",
      "epoch 2  | loss: 0.33613 | val_roc_auc_c: 0.80741 |  0:01:02s\n",
      "epoch 3  | loss: 0.30749 | val_roc_auc_c: 0.86843 |  0:01:22s\n",
      "epoch 4  | loss: 0.28221 | val_roc_auc_c: 0.88618 |  0:01:43s\n",
      "epoch 5  | loss: 0.26199 | val_roc_auc_c: 0.8925  |  0:02:04s\n",
      "epoch 6  | loss: 0.24935 | val_roc_auc_c: 0.89578 |  0:02:24s\n",
      "epoch 7  | loss: 0.24108 | val_roc_auc_c: 0.89887 |  0:02:44s\n",
      "epoch 8  | loss: 0.23348 | val_roc_auc_c: 0.90294 |  0:03:06s\n",
      "epoch 9  | loss: 0.22709 | val_roc_auc_c: 0.90395 |  0:03:26s\n",
      "epoch 10 | loss: 0.22278 | val_roc_auc_c: 0.90447 |  0:03:47s\n",
      "epoch 11 | loss: 0.21846 | val_roc_auc_c: 0.90552 |  0:04:07s\n",
      "epoch 12 | loss: 0.2142  | val_roc_auc_c: 0.90542 |  0:04:28s\n",
      "epoch 13 | loss: 0.21076 | val_roc_auc_c: 0.90693 |  0:04:48s\n",
      "epoch 14 | loss: 0.2068  | val_roc_auc_c: 0.90648 |  0:05:09s\n",
      "epoch 15 | loss: 0.20531 | val_roc_auc_c: 0.90549 |  0:05:30s\n",
      "epoch 16 | loss: 0.20149 | val_roc_auc_c: 0.90754 |  0:05:51s\n",
      "epoch 17 | loss: 0.19791 | val_roc_auc_c: 0.90723 |  0:06:12s\n",
      "epoch 18 | loss: 0.19621 | val_roc_auc_c: 0.91027 |  0:06:32s\n",
      "epoch 19 | loss: 0.19295 | val_roc_auc_c: 0.91197 |  0:06:52s\n",
      "epoch 20 | loss: 0.1917  | val_roc_auc_c: 0.91084 |  0:07:13s\n",
      "epoch 21 | loss: 0.1895  | val_roc_auc_c: 0.91073 |  0:07:34s\n",
      "epoch 22 | loss: 0.18692 | val_roc_auc_c: 0.9111  |  0:07:55s\n",
      "epoch 23 | loss: 0.18534 | val_roc_auc_c: 0.91299 |  0:08:16s\n",
      "epoch 24 | loss: 0.18276 | val_roc_auc_c: 0.91285 |  0:08:36s\n",
      "epoch 25 | loss: 0.18249 | val_roc_auc_c: 0.9117  |  0:08:57s\n",
      "epoch 26 | loss: 0.17983 | val_roc_auc_c: 0.91301 |  0:09:18s\n",
      "epoch 27 | loss: 0.17947 | val_roc_auc_c: 0.9143  |  0:09:39s\n",
      "epoch 28 | loss: 0.17743 | val_roc_auc_c: 0.913   |  0:10:00s\n",
      "epoch 29 | loss: 0.17598 | val_roc_auc_c: 0.91483 |  0:10:22s\n",
      "epoch 30 | loss: 0.17471 | val_roc_auc_c: 0.91433 |  0:10:43s\n",
      "epoch 31 | loss: 0.17288 | val_roc_auc_c: 0.91315 |  0:11:04s\n",
      "epoch 32 | loss: 0.17156 | val_roc_auc_c: 0.91346 |  0:11:25s\n",
      "epoch 33 | loss: 0.17195 | val_roc_auc_c: 0.91237 |  0:11:46s\n",
      "Epoch    34: reducing learning rate of group 0 to 6.6000e-04.\n",
      "epoch 34 | loss: 0.16542 | val_roc_auc_c: 0.91609 |  0:12:07s\n",
      "epoch 35 | loss: 0.16259 | val_roc_auc_c: 0.91649 |  0:12:28s\n",
      "epoch 36 | loss: 0.16031 | val_roc_auc_c: 0.91626 |  0:12:49s\n",
      "epoch 37 | loss: 0.15913 | val_roc_auc_c: 0.91628 |  0:13:09s\n",
      "epoch 38 | loss: 0.15878 | val_roc_auc_c: 0.91605 |  0:13:30s\n",
      "epoch 39 | loss: 0.15786 | val_roc_auc_c: 0.91688 |  0:13:51s\n",
      "epoch 40 | loss: 0.15711 | val_roc_auc_c: 0.91782 |  0:14:12s\n",
      "epoch 41 | loss: 0.15646 | val_roc_auc_c: 0.91748 |  0:14:33s\n",
      "epoch 42 | loss: 0.15588 | val_roc_auc_c: 0.91796 |  0:14:54s\n",
      "epoch 43 | loss: 0.15588 | val_roc_auc_c: 0.91725 |  0:15:15s\n",
      "epoch 44 | loss: 0.15547 | val_roc_auc_c: 0.91687 |  0:15:36s\n",
      "epoch 45 | loss: 0.15503 | val_roc_auc_c: 0.91633 |  0:15:57s\n",
      "epoch 46 | loss: 0.15447 | val_roc_auc_c: 0.91658 |  0:16:18s\n",
      "Epoch    47: reducing learning rate of group 0 to 2.1780e-04.\n",
      "epoch 47 | loss: 0.15193 | val_roc_auc_c: 0.91712 |  0:16:39s\n",
      "epoch 48 | loss: 0.15273 | val_roc_auc_c: 0.91689 |  0:17:00s\n",
      "epoch 49 | loss: 0.15159 | val_roc_auc_c: 0.91778 |  0:17:21s\n",
      "epoch 50 | loss: 0.15045 | val_roc_auc_c: 0.91749 |  0:17:41s\n",
      "Epoch    51: reducing learning rate of group 0 to 7.1874e-05.\n",
      "epoch 51 | loss: 0.15076 | val_roc_auc_c: 0.9174  |  0:18:02s\n",
      "epoch 52 | loss: 0.15026 | val_roc_auc_c: 0.91791 |  0:18:22s\n",
      "epoch 53 | loss: 0.14984 | val_roc_auc_c: 0.91769 |  0:18:43s\n",
      "epoch 54 | loss: 0.15053 | val_roc_auc_c: 0.91691 |  0:19:04s\n",
      "Epoch    55: reducing learning rate of group 0 to 2.3718e-05.\n",
      "epoch 55 | loss: 0.14942 | val_roc_auc_c: 0.91793 |  0:19:24s\n",
      "epoch 56 | loss: 0.14929 | val_roc_auc_c: 0.91808 |  0:19:45s\n",
      "epoch 57 | loss: 0.14886 | val_roc_auc_c: 0.91779 |  0:20:06s\n",
      "epoch 58 | loss: 0.14969 | val_roc_auc_c: 0.91755 |  0:20:27s\n",
      "epoch 59 | loss: 0.14926 | val_roc_auc_c: 0.91789 |  0:20:48s\n",
      "epoch 60 | loss: 0.15057 | val_roc_auc_c: 0.91796 |  0:21:09s\n",
      "Epoch    61: reducing learning rate of group 0 to 7.8271e-06.\n",
      "epoch 61 | loss: 0.14911 | val_roc_auc_c: 0.91763 |  0:21:29s\n",
      "epoch 62 | loss: 0.14993 | val_roc_auc_c: 0.91782 |  0:21:49s\n",
      "epoch 63 | loss: 0.14965 | val_roc_auc_c: 0.91799 |  0:22:09s\n",
      "epoch 64 | loss: 0.14927 | val_roc_auc_c: 0.91742 |  0:22:30s\n",
      "Epoch    65: reducing learning rate of group 0 to 2.5829e-06.\n",
      "epoch 65 | loss: 0.14874 | val_roc_auc_c: 0.91824 |  0:22:50s\n",
      "epoch 66 | loss: 0.14955 | val_roc_auc_c: 0.91791 |  0:23:10s\n",
      "epoch 67 | loss: 0.14988 | val_roc_auc_c: 0.91768 |  0:23:30s\n",
      "epoch 68 | loss: 0.1499  | val_roc_auc_c: 0.91828 |  0:23:50s\n",
      "epoch 69 | loss: 0.14941 | val_roc_auc_c: 0.91808 |  0:24:11s\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch 70 | loss: 0.14966 | val_roc_auc_c: 0.91775 |  0:24:31s\n",
      "epoch 71 | loss: 0.1493  | val_roc_auc_c: 0.91758 |  0:24:51s\n",
      "epoch 72 | loss: 0.14914 | val_roc_auc_c: 0.91786 |  0:25:11s\n",
      "epoch 73 | loss: 0.14876 | val_roc_auc_c: 0.9179  |  0:25:32s\n",
      "epoch 74 | loss: 0.1493  | val_roc_auc_c: 0.91745 |  0:25:52s\n",
      "epoch 75 | loss: 0.14939 | val_roc_auc_c: 0.91822 |  0:26:12s\n",
      "epoch 76 | loss: 0.14936 | val_roc_auc_c: 0.91842 |  0:26:32s\n",
      "epoch 77 | loss: 0.14894 | val_roc_auc_c: 0.91794 |  0:26:52s\n",
      "epoch 78 | loss: 0.14981 | val_roc_auc_c: 0.91819 |  0:27:12s\n",
      "epoch 79 | loss: 0.14948 | val_roc_auc_c: 0.91763 |  0:27:32s\n",
      "epoch 80 | loss: 0.14917 | val_roc_auc_c: 0.91776 |  0:27:52s\n",
      "epoch 81 | loss: 0.14925 | val_roc_auc_c: 0.91771 |  0:28:13s\n",
      "epoch 82 | loss: 0.1497  | val_roc_auc_c: 0.91788 |  0:28:32s\n",
      "epoch 83 | loss: 0.14891 | val_roc_auc_c: 0.91792 |  0:28:53s\n",
      "epoch 84 | loss: 0.14887 | val_roc_auc_c: 0.91808 |  0:29:13s\n",
      "epoch 85 | loss: 0.14939 | val_roc_auc_c: 0.91764 |  0:29:33s\n",
      "epoch 86 | loss: 0.1493  | val_roc_auc_c: 0.91814 |  0:29:54s\n",
      "epoch 87 | loss: 0.14913 | val_roc_auc_c: 0.91789 |  0:30:14s\n",
      "epoch 88 | loss: 0.1503  | val_roc_auc_c: 0.91788 |  0:30:35s\n",
      "epoch 89 | loss: 0.14866 | val_roc_auc_c: 0.918   |  0:30:56s\n",
      "epoch 90 | loss: 0.14906 | val_roc_auc_c: 0.91783 |  0:31:17s\n",
      "epoch 91 | loss: 0.15028 | val_roc_auc_c: 0.91802 |  0:31:38s\n",
      "epoch 92 | loss: 0.1488  | val_roc_auc_c: 0.91769 |  0:31:59s\n",
      "epoch 93 | loss: 0.14874 | val_roc_auc_c: 0.9182  |  0:32:20s\n",
      "epoch 94 | loss: 0.14908 | val_roc_auc_c: 0.91792 |  0:32:41s\n",
      "epoch 95 | loss: 0.14861 | val_roc_auc_c: 0.91808 |  0:33:02s\n",
      "epoch 96 | loss: 0.14938 | val_roc_auc_c: 0.91781 |  0:33:22s\n",
      "\n",
      "Early stopping occured at epoch 96 with best_epoch = 76 and best_val_roc_auc_c = 0.91842\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "\u001b[34m FOLDS:  \u001b[31m 4\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40242 | val_roc_auc_c: 0.53476 |  0:00:20s\n",
      "epoch 1  | loss: 0.35896 | val_roc_auc_c: 0.7057  |  0:00:41s\n",
      "epoch 2  | loss: 0.3364  | val_roc_auc_c: 0.81477 |  0:01:02s\n",
      "epoch 3  | loss: 0.30592 | val_roc_auc_c: 0.86522 |  0:01:23s\n",
      "epoch 4  | loss: 0.27898 | val_roc_auc_c: 0.88719 |  0:01:44s\n",
      "epoch 5  | loss: 0.26259 | val_roc_auc_c: 0.89482 |  0:02:06s\n",
      "epoch 6  | loss: 0.25018 | val_roc_auc_c: 0.89909 |  0:02:26s\n",
      "epoch 7  | loss: 0.24148 | val_roc_auc_c: 0.90021 |  0:02:48s\n",
      "epoch 8  | loss: 0.2349  | val_roc_auc_c: 0.90233 |  0:03:09s\n",
      "epoch 9  | loss: 0.22927 | val_roc_auc_c: 0.90295 |  0:03:29s\n",
      "epoch 10 | loss: 0.22327 | val_roc_auc_c: 0.90532 |  0:03:51s\n",
      "epoch 11 | loss: 0.22031 | val_roc_auc_c: 0.90461 |  0:04:12s\n",
      "epoch 12 | loss: 0.21547 | val_roc_auc_c: 0.90487 |  0:04:33s\n",
      "epoch 13 | loss: 0.21235 | val_roc_auc_c: 0.90695 |  0:04:54s\n",
      "epoch 14 | loss: 0.20882 | val_roc_auc_c: 0.9087  |  0:05:15s\n",
      "epoch 15 | loss: 0.20575 | val_roc_auc_c: 0.90729 |  0:05:36s\n",
      "epoch 16 | loss: 0.20253 | val_roc_auc_c: 0.91004 |  0:05:57s\n",
      "epoch 17 | loss: 0.19949 | val_roc_auc_c: 0.91053 |  0:06:18s\n",
      "epoch 18 | loss: 0.19751 | val_roc_auc_c: 0.90852 |  0:06:39s\n",
      "epoch 19 | loss: 0.19432 | val_roc_auc_c: 0.91018 |  0:07:01s\n",
      "epoch 20 | loss: 0.19237 | val_roc_auc_c: 0.9094  |  0:07:22s\n",
      "epoch 21 | loss: 0.19015 | val_roc_auc_c: 0.90951 |  0:07:43s\n",
      "Epoch    22: reducing learning rate of group 0 to 6.6000e-04.\n",
      "epoch 22 | loss: 0.18448 | val_roc_auc_c: 0.91312 |  0:08:05s\n",
      "epoch 23 | loss: 0.17931 | val_roc_auc_c: 0.91341 |  0:08:26s\n",
      "epoch 24 | loss: 0.17799 | val_roc_auc_c: 0.91319 |  0:08:48s\n",
      "epoch 25 | loss: 0.1764  | val_roc_auc_c: 0.91341 |  0:09:09s\n",
      "epoch 26 | loss: 0.17542 | val_roc_auc_c: 0.91511 |  0:09:30s\n",
      "epoch 27 | loss: 0.174   | val_roc_auc_c: 0.91398 |  0:09:52s\n",
      "epoch 28 | loss: 0.17324 | val_roc_auc_c: 0.91523 |  0:10:13s\n",
      "epoch 29 | loss: 0.17325 | val_roc_auc_c: 0.91419 |  0:10:34s\n",
      "epoch 30 | loss: 0.17221 | val_roc_auc_c: 0.91457 |  0:10:56s\n",
      "epoch 31 | loss: 0.17173 | val_roc_auc_c: 0.91472 |  0:11:17s\n",
      "epoch 32 | loss: 0.17    | val_roc_auc_c: 0.9136  |  0:11:38s\n",
      "Epoch    33: reducing learning rate of group 0 to 2.1780e-04.\n",
      "epoch 33 | loss: 0.16815 | val_roc_auc_c: 0.9148  |  0:12:00s\n",
      "epoch 34 | loss: 0.16733 | val_roc_auc_c: 0.91514 |  0:12:22s\n",
      "epoch 35 | loss: 0.16793 | val_roc_auc_c: 0.91503 |  0:12:42s\n",
      "epoch 36 | loss: 0.16625 | val_roc_auc_c: 0.91509 |  0:13:04s\n",
      "Epoch    37: reducing learning rate of group 0 to 7.1874e-05.\n",
      "epoch 37 | loss: 0.16612 | val_roc_auc_c: 0.91528 |  0:13:25s\n",
      "epoch 38 | loss: 0.16606 | val_roc_auc_c: 0.91561 |  0:13:46s\n",
      "epoch 39 | loss: 0.1657  | val_roc_auc_c: 0.91543 |  0:14:08s\n",
      "epoch 40 | loss: 0.16535 | val_roc_auc_c: 0.91563 |  0:14:29s\n",
      "epoch 41 | loss: 0.16499 | val_roc_auc_c: 0.91563 |  0:14:50s\n",
      "epoch 42 | loss: 0.16515 | val_roc_auc_c: 0.91591 |  0:15:11s\n",
      "epoch 43 | loss: 0.16474 | val_roc_auc_c: 0.91568 |  0:15:32s\n",
      "epoch 44 | loss: 0.16424 | val_roc_auc_c: 0.91586 |  0:15:53s\n",
      "epoch 45 | loss: 0.16524 | val_roc_auc_c: 0.91526 |  0:16:14s\n",
      "epoch 46 | loss: 0.16438 | val_roc_auc_c: 0.91585 |  0:16:35s\n",
      "Epoch    47: reducing learning rate of group 0 to 2.3718e-05.\n",
      "epoch 47 | loss: 0.16377 | val_roc_auc_c: 0.91583 |  0:16:56s\n",
      "epoch 48 | loss: 0.16461 | val_roc_auc_c: 0.91585 |  0:17:17s\n",
      "epoch 49 | loss: 0.16434 | val_roc_auc_c: 0.91601 |  0:17:38s\n",
      "epoch 50 | loss: 0.16372 | val_roc_auc_c: 0.91591 |  0:17:59s\n",
      "epoch 51 | loss: 0.16389 | val_roc_auc_c: 0.91563 |  0:18:20s\n",
      "epoch 52 | loss: 0.16316 | val_roc_auc_c: 0.91565 |  0:18:41s\n",
      "epoch 53 | loss: 0.16322 | val_roc_auc_c: 0.91603 |  0:19:02s\n",
      "Epoch    54: reducing learning rate of group 0 to 7.8271e-06.\n",
      "epoch 54 | loss: 0.16347 | val_roc_auc_c: 0.91657 |  0:19:23s\n",
      "epoch 55 | loss: 0.16354 | val_roc_auc_c: 0.91563 |  0:19:44s\n",
      "epoch 56 | loss: 0.1624  | val_roc_auc_c: 0.91579 |  0:20:05s\n",
      "epoch 57 | loss: 0.16437 | val_roc_auc_c: 0.91662 |  0:20:26s\n",
      "epoch 58 | loss: 0.16378 | val_roc_auc_c: 0.91576 |  0:20:47s\n",
      "Epoch    59: reducing learning rate of group 0 to 2.5829e-06.\n",
      "epoch 59 | loss: 0.16452 | val_roc_auc_c: 0.91602 |  0:21:08s\n",
      "epoch 60 | loss: 0.16331 | val_roc_auc_c: 0.91577 |  0:21:29s\n",
      "epoch 61 | loss: 0.16349 | val_roc_auc_c: 0.91566 |  0:21:50s\n",
      "epoch 62 | loss: 0.16315 | val_roc_auc_c: 0.91602 |  0:22:11s\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch 63 | loss: 0.16424 | val_roc_auc_c: 0.9161  |  0:22:32s\n",
      "epoch 64 | loss: 0.16304 | val_roc_auc_c: 0.91591 |  0:22:53s\n",
      "epoch 65 | loss: 0.16304 | val_roc_auc_c: 0.91637 |  0:23:14s\n",
      "epoch 66 | loss: 0.16304 | val_roc_auc_c: 0.91539 |  0:23:35s\n",
      "epoch 67 | loss: 0.16371 | val_roc_auc_c: 0.91591 |  0:23:56s\n",
      "epoch 68 | loss: 0.16298 | val_roc_auc_c: 0.91613 |  0:24:17s\n",
      "epoch 69 | loss: 0.16303 | val_roc_auc_c: 0.91543 |  0:24:38s\n",
      "epoch 70 | loss: 0.16372 | val_roc_auc_c: 0.91595 |  0:24:59s\n",
      "epoch 71 | loss: 0.16351 | val_roc_auc_c: 0.91563 |  0:25:20s\n",
      "epoch 72 | loss: 0.16287 | val_roc_auc_c: 0.91578 |  0:25:41s\n",
      "epoch 73 | loss: 0.16307 | val_roc_auc_c: 0.91573 |  0:26:02s\n",
      "epoch 74 | loss: 0.16337 | val_roc_auc_c: 0.916   |  0:26:23s\n",
      "epoch 75 | loss: 0.16302 | val_roc_auc_c: 0.91573 |  0:26:44s\n",
      "epoch 76 | loss: 0.16374 | val_roc_auc_c: 0.91566 |  0:27:05s\n",
      "epoch 77 | loss: 0.16331 | val_roc_auc_c: 0.91609 |  0:27:26s\n",
      "\n",
      "Early stopping occured at epoch 77 with best_epoch = 57 and best_val_roc_auc_c = 0.91662\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "\u001b[34m FOLDS:  \u001b[31m 5\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40261 | val_roc_auc_c: 0.53529 |  0:00:21s\n",
      "epoch 1  | loss: 0.35874 | val_roc_auc_c: 0.69842 |  0:00:42s\n",
      "epoch 2  | loss: 0.33553 | val_roc_auc_c: 0.81521 |  0:01:03s\n",
      "epoch 3  | loss: 0.30707 | val_roc_auc_c: 0.8693  |  0:01:24s\n",
      "epoch 4  | loss: 0.27999 | val_roc_auc_c: 0.88566 |  0:01:45s\n",
      "epoch 5  | loss: 0.26427 | val_roc_auc_c: 0.89236 |  0:02:06s\n",
      "epoch 6  | loss: 0.25006 | val_roc_auc_c: 0.89555 |  0:02:27s\n",
      "epoch 7  | loss: 0.24225 | val_roc_auc_c: 0.89841 |  0:02:48s\n",
      "epoch 8  | loss: 0.23519 | val_roc_auc_c: 0.89914 |  0:03:09s\n",
      "epoch 9  | loss: 0.22945 | val_roc_auc_c: 0.90236 |  0:03:30s\n",
      "epoch 10 | loss: 0.2231  | val_roc_auc_c: 0.90371 |  0:03:51s\n",
      "epoch 11 | loss: 0.21983 | val_roc_auc_c: 0.90546 |  0:04:12s\n",
      "epoch 12 | loss: 0.21609 | val_roc_auc_c: 0.90387 |  0:04:33s\n",
      "epoch 13 | loss: 0.21206 | val_roc_auc_c: 0.9071  |  0:04:54s\n",
      "epoch 14 | loss: 0.20929 | val_roc_auc_c: 0.90819 |  0:05:16s\n",
      "epoch 15 | loss: 0.20545 | val_roc_auc_c: 0.90881 |  0:05:37s\n",
      "epoch 16 | loss: 0.20246 | val_roc_auc_c: 0.91002 |  0:05:58s\n",
      "epoch 17 | loss: 0.19868 | val_roc_auc_c: 0.91339 |  0:06:20s\n",
      "epoch 18 | loss: 0.19703 | val_roc_auc_c: 0.91017 |  0:06:41s\n",
      "epoch 19 | loss: 0.19421 | val_roc_auc_c: 0.91398 |  0:07:03s\n",
      "epoch 20 | loss: 0.19268 | val_roc_auc_c: 0.91069 |  0:07:25s\n",
      "epoch 21 | loss: 0.1904  | val_roc_auc_c: 0.91341 |  0:07:46s\n",
      "epoch 22 | loss: 0.18925 | val_roc_auc_c: 0.91364 |  0:08:08s\n",
      "epoch 23 | loss: 0.1865  | val_roc_auc_c: 0.91416 |  0:08:30s\n",
      "epoch 24 | loss: 0.18459 | val_roc_auc_c: 0.91356 |  0:08:51s\n",
      "epoch 25 | loss: 0.18208 | val_roc_auc_c: 0.91553 |  0:09:13s\n",
      "epoch 26 | loss: 0.18226 | val_roc_auc_c: 0.91418 |  0:09:34s\n",
      "epoch 27 | loss: 0.17944 | val_roc_auc_c: 0.91524 |  0:09:56s\n",
      "epoch 28 | loss: 0.17736 | val_roc_auc_c: 0.91367 |  0:10:18s\n",
      "epoch 29 | loss: 0.17624 | val_roc_auc_c: 0.91382 |  0:10:39s\n",
      "Epoch    30: reducing learning rate of group 0 to 6.6000e-04.\n",
      "epoch 30 | loss: 0.17073 | val_roc_auc_c: 0.91646 |  0:11:00s\n",
      "epoch 31 | loss: 0.16663 | val_roc_auc_c: 0.91591 |  0:11:20s\n",
      "epoch 32 | loss: 0.16561 | val_roc_auc_c: 0.91658 |  0:11:41s\n",
      "epoch 33 | loss: 0.16473 | val_roc_auc_c: 0.9164  |  0:12:02s\n",
      "epoch 34 | loss: 0.16368 | val_roc_auc_c: 0.91637 |  0:12:23s\n",
      "epoch 35 | loss: 0.16299 | val_roc_auc_c: 0.9156  |  0:12:44s\n",
      "epoch 36 | loss: 0.16172 | val_roc_auc_c: 0.91656 |  0:13:04s\n",
      "Epoch    37: reducing learning rate of group 0 to 2.1780e-04.\n",
      "epoch 37 | loss: 0.15958 | val_roc_auc_c: 0.91737 |  0:13:24s\n",
      "epoch 38 | loss: 0.15959 | val_roc_auc_c: 0.917   |  0:13:45s\n",
      "epoch 39 | loss: 0.15867 | val_roc_auc_c: 0.91744 |  0:14:05s\n",
      "epoch 40 | loss: 0.15918 | val_roc_auc_c: 0.91769 |  0:14:26s\n",
      "epoch 41 | loss: 0.15799 | val_roc_auc_c: 0.91792 |  0:14:47s\n",
      "epoch 42 | loss: 0.15767 | val_roc_auc_c: 0.91803 |  0:15:07s\n",
      "epoch 43 | loss: 0.15733 | val_roc_auc_c: 0.91714 |  0:15:28s\n",
      "epoch 44 | loss: 0.15638 | val_roc_auc_c: 0.91812 |  0:15:49s\n",
      "epoch 45 | loss: 0.15746 | val_roc_auc_c: 0.91732 |  0:16:09s\n",
      "epoch 46 | loss: 0.15633 | val_roc_auc_c: 0.91773 |  0:16:30s\n",
      "epoch 47 | loss: 0.15661 | val_roc_auc_c: 0.91805 |  0:16:50s\n",
      "epoch 48 | loss: 0.15577 | val_roc_auc_c: 0.91837 |  0:17:11s\n",
      "epoch 49 | loss: 0.15593 | val_roc_auc_c: 0.9176  |  0:17:32s\n",
      "epoch 50 | loss: 0.15523 | val_roc_auc_c: 0.91768 |  0:17:53s\n",
      "epoch 51 | loss: 0.15559 | val_roc_auc_c: 0.91787 |  0:18:14s\n",
      "epoch 52 | loss: 0.15573 | val_roc_auc_c: 0.91825 |  0:18:35s\n",
      "Epoch    53: reducing learning rate of group 0 to 7.1874e-05.\n",
      "epoch 53 | loss: 0.15449 | val_roc_auc_c: 0.91813 |  0:18:56s\n",
      "epoch 54 | loss: 0.15411 | val_roc_auc_c: 0.91775 |  0:19:17s\n",
      "epoch 55 | loss: 0.15404 | val_roc_auc_c: 0.91784 |  0:19:38s\n",
      "epoch 56 | loss: 0.15432 | val_roc_auc_c: 0.91775 |  0:19:59s\n",
      "Epoch    57: reducing learning rate of group 0 to 2.3718e-05.\n",
      "epoch 57 | loss: 0.15384 | val_roc_auc_c: 0.9179  |  0:20:20s\n",
      "epoch 58 | loss: 0.15372 | val_roc_auc_c: 0.91796 |  0:20:40s\n",
      "epoch 59 | loss: 0.15377 | val_roc_auc_c: 0.91819 |  0:21:01s\n",
      "epoch 60 | loss: 0.15434 | val_roc_auc_c: 0.91781 |  0:21:24s\n",
      "Epoch    61: reducing learning rate of group 0 to 7.8271e-06.\n",
      "epoch 61 | loss: 0.1535  | val_roc_auc_c: 0.91786 |  0:21:45s\n",
      "epoch 62 | loss: 0.15428 | val_roc_auc_c: 0.91762 |  0:22:06s\n",
      "epoch 63 | loss: 0.15347 | val_roc_auc_c: 0.9182  |  0:22:28s\n",
      "epoch 64 | loss: 0.15334 | val_roc_auc_c: 0.91827 |  0:22:49s\n",
      "Epoch    65: reducing learning rate of group 0 to 2.5829e-06.\n",
      "epoch 65 | loss: 0.15363 | val_roc_auc_c: 0.91823 |  0:23:09s\n",
      "epoch 66 | loss: 0.15367 | val_roc_auc_c: 0.91837 |  0:23:30s\n",
      "epoch 67 | loss: 0.15362 | val_roc_auc_c: 0.91802 |  0:23:50s\n",
      "epoch 68 | loss: 0.15407 | val_roc_auc_c: 0.91828 |  0:24:10s\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch 69 | loss: 0.15415 | val_roc_auc_c: 0.91807 |  0:24:31s\n",
      "epoch 70 | loss: 0.15381 | val_roc_auc_c: 0.91815 |  0:24:51s\n",
      "epoch 71 | loss: 0.15353 | val_roc_auc_c: 0.91769 |  0:25:11s\n",
      "epoch 72 | loss: 0.15322 | val_roc_auc_c: 0.91804 |  0:25:32s\n",
      "epoch 73 | loss: 0.15351 | val_roc_auc_c: 0.91806 |  0:25:52s\n",
      "epoch 74 | loss: 0.15403 | val_roc_auc_c: 0.91789 |  0:26:13s\n",
      "epoch 75 | loss: 0.15378 | val_roc_auc_c: 0.91786 |  0:26:34s\n",
      "epoch 76 | loss: 0.15342 | val_roc_auc_c: 0.91807 |  0:26:55s\n",
      "epoch 77 | loss: 0.15357 | val_roc_auc_c: 0.91794 |  0:27:15s\n",
      "epoch 78 | loss: 0.15358 | val_roc_auc_c: 0.91801 |  0:27:36s\n",
      "epoch 79 | loss: 0.15343 | val_roc_auc_c: 0.9181  |  0:27:56s\n",
      "epoch 80 | loss: 0.15347 | val_roc_auc_c: 0.91801 |  0:28:17s\n",
      "epoch 81 | loss: 0.15368 | val_roc_auc_c: 0.91804 |  0:28:38s\n",
      "epoch 82 | loss: 0.15332 | val_roc_auc_c: 0.91824 |  0:28:58s\n",
      "epoch 83 | loss: 0.15386 | val_roc_auc_c: 0.91764 |  0:29:19s\n",
      "epoch 84 | loss: 0.15387 | val_roc_auc_c: 0.91806 |  0:29:39s\n",
      "epoch 85 | loss: 0.1527  | val_roc_auc_c: 0.91791 |  0:30:00s\n",
      "epoch 86 | loss: 0.15348 | val_roc_auc_c: 0.91797 |  0:30:21s\n",
      "\n",
      "Early stopping occured at epoch 86 with best_epoch = 66 and best_val_roc_auc_c = 0.91837\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "\u001b[34m FOLDS:  \u001b[31m 6\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40178 | val_roc_auc_c: 0.53818 |  0:00:20s\n",
      "epoch 1  | loss: 0.35776 | val_roc_auc_c: 0.71495 |  0:00:40s\n",
      "epoch 2  | loss: 0.33343 | val_roc_auc_c: 0.81744 |  0:01:01s\n",
      "epoch 3  | loss: 0.30386 | val_roc_auc_c: 0.87072 |  0:01:21s\n",
      "epoch 4  | loss: 0.27688 | val_roc_auc_c: 0.88608 |  0:01:42s\n",
      "epoch 5  | loss: 0.25909 | val_roc_auc_c: 0.89323 |  0:02:03s\n",
      "epoch 6  | loss: 0.24789 | val_roc_auc_c: 0.89756 |  0:02:23s\n",
      "epoch 7  | loss: 0.23926 | val_roc_auc_c: 0.89831 |  0:02:44s\n",
      "epoch 8  | loss: 0.23238 | val_roc_auc_c: 0.89904 |  0:03:04s\n",
      "epoch 9  | loss: 0.2261  | val_roc_auc_c: 0.90016 |  0:03:25s\n",
      "epoch 10 | loss: 0.2214  | val_roc_auc_c: 0.90179 |  0:03:46s\n",
      "epoch 11 | loss: 0.21774 | val_roc_auc_c: 0.90285 |  0:04:07s\n",
      "epoch 12 | loss: 0.21226 | val_roc_auc_c: 0.90311 |  0:04:27s\n",
      "epoch 13 | loss: 0.20971 | val_roc_auc_c: 0.90477 |  0:04:48s\n",
      "epoch 14 | loss: 0.20672 | val_roc_auc_c: 0.90534 |  0:05:08s\n",
      "epoch 15 | loss: 0.20299 | val_roc_auc_c: 0.90522 |  0:05:29s\n",
      "epoch 16 | loss: 0.20065 | val_roc_auc_c: 0.90729 |  0:05:50s\n",
      "epoch 17 | loss: 0.19787 | val_roc_auc_c: 0.90603 |  0:06:11s\n",
      "epoch 18 | loss: 0.19509 | val_roc_auc_c: 0.90617 |  0:06:32s\n",
      "epoch 19 | loss: 0.19317 | val_roc_auc_c: 0.9074  |  0:06:53s\n",
      "epoch 20 | loss: 0.18949 | val_roc_auc_c: 0.90755 |  0:07:13s\n",
      "epoch 21 | loss: 0.18803 | val_roc_auc_c: 0.9077  |  0:07:34s\n",
      "epoch 22 | loss: 0.18729 | val_roc_auc_c: 0.90741 |  0:07:55s\n",
      "epoch 23 | loss: 0.18559 | val_roc_auc_c: 0.90936 |  0:08:15s\n",
      "epoch 24 | loss: 0.1834  | val_roc_auc_c: 0.90907 |  0:08:35s\n",
      "epoch 25 | loss: 0.1813  | val_roc_auc_c: 0.90885 |  0:08:56s\n",
      "epoch 26 | loss: 0.18058 | val_roc_auc_c: 0.91015 |  0:09:16s\n",
      "epoch 27 | loss: 0.17833 | val_roc_auc_c: 0.91097 |  0:09:37s\n",
      "epoch 28 | loss: 0.17582 | val_roc_auc_c: 0.90982 |  0:09:58s\n",
      "epoch 29 | loss: 0.17579 | val_roc_auc_c: 0.90775 |  0:10:19s\n",
      "epoch 30 | loss: 0.17526 | val_roc_auc_c: 0.91067 |  0:10:40s\n",
      "epoch 31 | loss: 0.17343 | val_roc_auc_c: 0.90891 |  0:11:00s\n",
      "Epoch    32: reducing learning rate of group 0 to 6.6000e-04.\n",
      "epoch 32 | loss: 0.16694 | val_roc_auc_c: 0.91137 |  0:11:21s\n",
      "epoch 33 | loss: 0.16412 | val_roc_auc_c: 0.91098 |  0:11:41s\n",
      "epoch 34 | loss: 0.16274 | val_roc_auc_c: 0.91203 |  0:12:02s\n",
      "epoch 35 | loss: 0.16151 | val_roc_auc_c: 0.9128  |  0:12:22s\n",
      "epoch 36 | loss: 0.16079 | val_roc_auc_c: 0.91244 |  0:12:42s\n",
      "epoch 37 | loss: 0.16012 | val_roc_auc_c: 0.91241 |  0:13:03s\n",
      "epoch 38 | loss: 0.15898 | val_roc_auc_c: 0.91229 |  0:13:24s\n",
      "epoch 39 | loss: 0.15828 | val_roc_auc_c: 0.91221 |  0:13:44s\n",
      "Epoch    40: reducing learning rate of group 0 to 2.1780e-04.\n",
      "epoch 40 | loss: 0.15663 | val_roc_auc_c: 0.91334 |  0:14:05s\n",
      "epoch 41 | loss: 0.15683 | val_roc_auc_c: 0.91319 |  0:14:26s\n",
      "epoch 42 | loss: 0.15539 | val_roc_auc_c: 0.91329 |  0:14:46s\n",
      "epoch 43 | loss: 0.1554  | val_roc_auc_c: 0.91346 |  0:15:07s\n",
      "epoch 44 | loss: 0.15486 | val_roc_auc_c: 0.91315 |  0:15:28s\n",
      "epoch 45 | loss: 0.15436 | val_roc_auc_c: 0.91306 |  0:15:49s\n",
      "epoch 46 | loss: 0.15415 | val_roc_auc_c: 0.91331 |  0:16:09s\n",
      "epoch 47 | loss: 0.1532  | val_roc_auc_c: 0.91317 |  0:16:29s\n",
      "Epoch    48: reducing learning rate of group 0 to 7.1874e-05.\n",
      "epoch 48 | loss: 0.15287 | val_roc_auc_c: 0.91353 |  0:16:50s\n",
      "epoch 49 | loss: 0.15324 | val_roc_auc_c: 0.91376 |  0:17:11s\n",
      "epoch 50 | loss: 0.15241 | val_roc_auc_c: 0.91349 |  0:17:31s\n",
      "epoch 51 | loss: 0.15291 | val_roc_auc_c: 0.91397 |  0:17:52s\n",
      "epoch 52 | loss: 0.15285 | val_roc_auc_c: 0.9137  |  0:18:13s\n",
      "epoch 53 | loss: 0.15363 | val_roc_auc_c: 0.91362 |  0:18:33s\n",
      "epoch 54 | loss: 0.15234 | val_roc_auc_c: 0.91391 |  0:18:54s\n",
      "epoch 55 | loss: 0.15245 | val_roc_auc_c: 0.91342 |  0:19:14s\n",
      "Epoch    56: reducing learning rate of group 0 to 2.3718e-05.\n",
      "epoch 56 | loss: 0.15164 | val_roc_auc_c: 0.91334 |  0:19:35s\n",
      "epoch 57 | loss: 0.15249 | val_roc_auc_c: 0.91399 |  0:19:56s\n",
      "epoch 58 | loss: 0.15255 | val_roc_auc_c: 0.91366 |  0:20:16s\n",
      "epoch 59 | loss: 0.15146 | val_roc_auc_c: 0.91331 |  0:20:37s\n",
      "Epoch    60: reducing learning rate of group 0 to 7.8271e-06.\n",
      "epoch 60 | loss: 0.15188 | val_roc_auc_c: 0.91378 |  0:20:57s\n",
      "epoch 61 | loss: 0.15154 | val_roc_auc_c: 0.91396 |  0:21:18s\n",
      "epoch 62 | loss: 0.15213 | val_roc_auc_c: 0.9139  |  0:21:38s\n",
      "epoch 63 | loss: 0.15181 | val_roc_auc_c: 0.91378 |  0:21:59s\n",
      "Epoch    64: reducing learning rate of group 0 to 2.5829e-06.\n",
      "epoch 64 | loss: 0.15186 | val_roc_auc_c: 0.91376 |  0:22:19s\n",
      "epoch 65 | loss: 0.15159 | val_roc_auc_c: 0.91397 |  0:22:40s\n",
      "epoch 66 | loss: 0.15219 | val_roc_auc_c: 0.91391 |  0:23:01s\n",
      "epoch 67 | loss: 0.15191 | val_roc_auc_c: 0.91386 |  0:23:22s\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch 68 | loss: 0.15107 | val_roc_auc_c: 0.91393 |  0:23:43s\n",
      "epoch 69 | loss: 0.15173 | val_roc_auc_c: 0.91425 |  0:24:03s\n",
      "epoch 70 | loss: 0.15185 | val_roc_auc_c: 0.9138  |  0:24:24s\n",
      "epoch 71 | loss: 0.15137 | val_roc_auc_c: 0.91361 |  0:24:45s\n",
      "epoch 72 | loss: 0.15288 | val_roc_auc_c: 0.91377 |  0:25:05s\n",
      "epoch 73 | loss: 0.15231 | val_roc_auc_c: 0.91387 |  0:25:26s\n",
      "epoch 74 | loss: 0.15184 | val_roc_auc_c: 0.91389 |  0:25:48s\n",
      "epoch 75 | loss: 0.15153 | val_roc_auc_c: 0.91369 |  0:26:09s\n",
      "epoch 76 | loss: 0.15187 | val_roc_auc_c: 0.91363 |  0:26:30s\n",
      "epoch 77 | loss: 0.15161 | val_roc_auc_c: 0.91401 |  0:26:52s\n",
      "epoch 78 | loss: 0.15199 | val_roc_auc_c: 0.91367 |  0:27:13s\n",
      "epoch 79 | loss: 0.15134 | val_roc_auc_c: 0.91392 |  0:27:34s\n",
      "epoch 80 | loss: 0.15191 | val_roc_auc_c: 0.91333 |  0:27:55s\n",
      "epoch 81 | loss: 0.15208 | val_roc_auc_c: 0.91394 |  0:28:16s\n",
      "epoch 82 | loss: 0.15232 | val_roc_auc_c: 0.91377 |  0:28:37s\n",
      "epoch 83 | loss: 0.15144 | val_roc_auc_c: 0.91401 |  0:28:59s\n",
      "epoch 84 | loss: 0.1522  | val_roc_auc_c: 0.91394 |  0:29:19s\n",
      "epoch 85 | loss: 0.1521  | val_roc_auc_c: 0.91418 |  0:29:40s\n",
      "epoch 86 | loss: 0.15109 | val_roc_auc_c: 0.91389 |  0:30:01s\n",
      "epoch 87 | loss: 0.15151 | val_roc_auc_c: 0.91362 |  0:30:22s\n",
      "epoch 88 | loss: 0.15211 | val_roc_auc_c: 0.91342 |  0:30:43s\n",
      "epoch 89 | loss: 0.15206 | val_roc_auc_c: 0.91397 |  0:31:04s\n",
      "\n",
      "Early stopping occured at epoch 89 with best_epoch = 69 and best_val_roc_auc_c = 0.91425\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "\u001b[34m FOLDS:  \u001b[31m 7\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40162 | val_roc_auc_c: 0.54571 |  0:00:21s\n",
      "epoch 1  | loss: 0.35838 | val_roc_auc_c: 0.71848 |  0:00:41s\n",
      "epoch 2  | loss: 0.33483 | val_roc_auc_c: 0.81659 |  0:01:03s\n",
      "epoch 3  | loss: 0.3039  | val_roc_auc_c: 0.86586 |  0:01:24s\n",
      "epoch 4  | loss: 0.27808 | val_roc_auc_c: 0.88274 |  0:01:45s\n",
      "epoch 5  | loss: 0.26057 | val_roc_auc_c: 0.88945 |  0:02:06s\n",
      "epoch 6  | loss: 0.24876 | val_roc_auc_c: 0.89306 |  0:02:27s\n",
      "epoch 7  | loss: 0.24006 | val_roc_auc_c: 0.89529 |  0:02:48s\n",
      "epoch 8  | loss: 0.23357 | val_roc_auc_c: 0.89621 |  0:03:09s\n",
      "epoch 9  | loss: 0.22794 | val_roc_auc_c: 0.89988 |  0:03:30s\n",
      "epoch 10 | loss: 0.2218  | val_roc_auc_c: 0.9009  |  0:03:51s\n",
      "epoch 11 | loss: 0.21858 | val_roc_auc_c: 0.90098 |  0:04:13s\n",
      "epoch 12 | loss: 0.2137  | val_roc_auc_c: 0.90257 |  0:04:33s\n",
      "epoch 13 | loss: 0.21043 | val_roc_auc_c: 0.90318 |  0:04:54s\n",
      "epoch 14 | loss: 0.20673 | val_roc_auc_c: 0.90515 |  0:05:15s\n",
      "epoch 15 | loss: 0.20379 | val_roc_auc_c: 0.90617 |  0:05:36s\n",
      "epoch 16 | loss: 0.20071 | val_roc_auc_c: 0.90566 |  0:05:58s\n",
      "epoch 17 | loss: 0.19755 | val_roc_auc_c: 0.90662 |  0:06:19s\n",
      "epoch 18 | loss: 0.19592 | val_roc_auc_c: 0.90687 |  0:06:40s\n",
      "epoch 19 | loss: 0.19413 | val_roc_auc_c: 0.90898 |  0:07:01s\n",
      "epoch 20 | loss: 0.1905  | val_roc_auc_c: 0.90905 |  0:07:23s\n",
      "epoch 21 | loss: 0.18891 | val_roc_auc_c: 0.90929 |  0:07:44s\n",
      "epoch 22 | loss: 0.18697 | val_roc_auc_c: 0.90868 |  0:08:05s\n",
      "epoch 23 | loss: 0.18543 | val_roc_auc_c: 0.90857 |  0:08:27s\n",
      "epoch 24 | loss: 0.1831  | val_roc_auc_c: 0.91133 |  0:08:48s\n",
      "epoch 25 | loss: 0.18107 | val_roc_auc_c: 0.91228 |  0:09:09s\n",
      "epoch 26 | loss: 0.17943 | val_roc_auc_c: 0.91229 |  0:09:31s\n",
      "epoch 27 | loss: 0.17768 | val_roc_auc_c: 0.91018 |  0:09:52s\n",
      "epoch 28 | loss: 0.1758  | val_roc_auc_c: 0.90952 |  0:10:13s\n",
      "epoch 29 | loss: 0.17599 | val_roc_auc_c: 0.91092 |  0:10:35s\n",
      "Epoch    30: reducing learning rate of group 0 to 6.6000e-04.\n",
      "epoch 30 | loss: 0.16911 | val_roc_auc_c: 0.91427 |  0:10:56s\n",
      "epoch 31 | loss: 0.16544 | val_roc_auc_c: 0.91536 |  0:11:18s\n",
      "epoch 32 | loss: 0.16484 | val_roc_auc_c: 0.91496 |  0:11:40s\n",
      "epoch 33 | loss: 0.1633  | val_roc_auc_c: 0.91536 |  0:12:00s\n",
      "epoch 34 | loss: 0.16244 | val_roc_auc_c: 0.91564 |  0:12:22s\n",
      "epoch 35 | loss: 0.1613  | val_roc_auc_c: 0.91577 |  0:12:43s\n",
      "epoch 36 | loss: 0.16007 | val_roc_auc_c: 0.91482 |  0:13:04s\n",
      "epoch 37 | loss: 0.1602  | val_roc_auc_c: 0.91514 |  0:13:26s\n",
      "epoch 38 | loss: 0.15992 | val_roc_auc_c: 0.91443 |  0:13:47s\n",
      "epoch 39 | loss: 0.15879 | val_roc_auc_c: 0.91418 |  0:14:08s\n",
      "Epoch    40: reducing learning rate of group 0 to 2.1780e-04.\n",
      "epoch 40 | loss: 0.15793 | val_roc_auc_c: 0.91456 |  0:14:30s\n",
      "epoch 41 | loss: 0.15591 | val_roc_auc_c: 0.9151  |  0:14:52s\n",
      "epoch 42 | loss: 0.1554  | val_roc_auc_c: 0.91591 |  0:15:13s\n",
      "epoch 43 | loss: 0.15466 | val_roc_auc_c: 0.91553 |  0:15:35s\n",
      "epoch 44 | loss: 0.15466 | val_roc_auc_c: 0.91533 |  0:15:56s\n",
      "epoch 45 | loss: 0.15466 | val_roc_auc_c: 0.91551 |  0:16:18s\n",
      "epoch 46 | loss: 0.15434 | val_roc_auc_c: 0.91527 |  0:16:40s\n",
      "Epoch    47: reducing learning rate of group 0 to 7.1874e-05.\n",
      "epoch 47 | loss: 0.15336 | val_roc_auc_c: 0.91557 |  0:17:01s\n",
      "epoch 48 | loss: 0.15257 | val_roc_auc_c: 0.9159  |  0:17:22s\n",
      "epoch 49 | loss: 0.15315 | val_roc_auc_c: 0.91534 |  0:17:44s\n",
      "epoch 50 | loss: 0.15337 | val_roc_auc_c: 0.91545 |  0:18:05s\n",
      "Epoch    51: reducing learning rate of group 0 to 2.3718e-05.\n",
      "epoch 51 | loss: 0.15219 | val_roc_auc_c: 0.9157  |  0:18:26s\n",
      "epoch 52 | loss: 0.15251 | val_roc_auc_c: 0.91559 |  0:18:48s\n",
      "epoch 53 | loss: 0.15308 | val_roc_auc_c: 0.91601 |  0:19:09s\n",
      "epoch 54 | loss: 0.15256 | val_roc_auc_c: 0.91596 |  0:19:31s\n",
      "epoch 55 | loss: 0.15275 | val_roc_auc_c: 0.91599 |  0:19:52s\n",
      "epoch 56 | loss: 0.15267 | val_roc_auc_c: 0.91565 |  0:20:13s\n",
      "epoch 57 | loss: 0.15218 | val_roc_auc_c: 0.91588 |  0:20:34s\n",
      "Epoch    58: reducing learning rate of group 0 to 7.8271e-06.\n",
      "epoch 58 | loss: 0.15229 | val_roc_auc_c: 0.91592 |  0:20:55s\n",
      "epoch 59 | loss: 0.15303 | val_roc_auc_c: 0.91566 |  0:21:16s\n",
      "epoch 60 | loss: 0.1524  | val_roc_auc_c: 0.9154  |  0:21:37s\n",
      "epoch 61 | loss: 0.1524  | val_roc_auc_c: 0.91631 |  0:21:59s\n",
      "epoch 62 | loss: 0.15223 | val_roc_auc_c: 0.91539 |  0:22:20s\n",
      "epoch 63 | loss: 0.15215 | val_roc_auc_c: 0.9152  |  0:22:41s\n",
      "epoch 64 | loss: 0.15254 | val_roc_auc_c: 0.91571 |  0:23:02s\n",
      "epoch 65 | loss: 0.15247 | val_roc_auc_c: 0.91569 |  0:23:23s\n",
      "Epoch    66: reducing learning rate of group 0 to 2.5829e-06.\n",
      "epoch 66 | loss: 0.15278 | val_roc_auc_c: 0.91536 |  0:23:44s\n",
      "epoch 67 | loss: 0.15253 | val_roc_auc_c: 0.91575 |  0:24:05s\n",
      "epoch 68 | loss: 0.15231 | val_roc_auc_c: 0.91589 |  0:24:26s\n",
      "epoch 69 | loss: 0.15229 | val_roc_auc_c: 0.91564 |  0:24:47s\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch 70 | loss: 0.15204 | val_roc_auc_c: 0.91534 |  0:25:08s\n",
      "epoch 71 | loss: 0.15186 | val_roc_auc_c: 0.91609 |  0:25:29s\n",
      "epoch 72 | loss: 0.15206 | val_roc_auc_c: 0.91581 |  0:25:50s\n",
      "epoch 73 | loss: 0.15245 | val_roc_auc_c: 0.91573 |  0:26:11s\n",
      "epoch 74 | loss: 0.15211 | val_roc_auc_c: 0.91583 |  0:26:32s\n",
      "epoch 75 | loss: 0.15209 | val_roc_auc_c: 0.91556 |  0:26:53s\n",
      "epoch 76 | loss: 0.15165 | val_roc_auc_c: 0.91603 |  0:27:14s\n",
      "epoch 77 | loss: 0.15202 | val_roc_auc_c: 0.91539 |  0:27:34s\n",
      "epoch 78 | loss: 0.15227 | val_roc_auc_c: 0.9161  |  0:27:56s\n",
      "epoch 79 | loss: 0.15239 | val_roc_auc_c: 0.91644 |  0:28:17s\n",
      "epoch 80 | loss: 0.15271 | val_roc_auc_c: 0.9163  |  0:28:38s\n",
      "epoch 81 | loss: 0.15246 | val_roc_auc_c: 0.91597 |  0:28:59s\n",
      "epoch 82 | loss: 0.15234 | val_roc_auc_c: 0.91517 |  0:29:20s\n",
      "epoch 83 | loss: 0.15195 | val_roc_auc_c: 0.9159  |  0:29:41s\n",
      "epoch 84 | loss: 0.15223 | val_roc_auc_c: 0.91595 |  0:30:02s\n",
      "epoch 85 | loss: 0.15229 | val_roc_auc_c: 0.91541 |  0:30:23s\n",
      "epoch 86 | loss: 0.15229 | val_roc_auc_c: 0.9153  |  0:30:44s\n",
      "epoch 87 | loss: 0.15137 | val_roc_auc_c: 0.91576 |  0:31:05s\n",
      "epoch 88 | loss: 0.15237 | val_roc_auc_c: 0.91618 |  0:31:26s\n",
      "epoch 89 | loss: 0.152   | val_roc_auc_c: 0.91628 |  0:31:47s\n",
      "epoch 90 | loss: 0.15231 | val_roc_auc_c: 0.91583 |  0:32:08s\n",
      "epoch 91 | loss: 0.15206 | val_roc_auc_c: 0.91587 |  0:32:29s\n",
      "epoch 92 | loss: 0.15116 | val_roc_auc_c: 0.91576 |  0:32:50s\n",
      "epoch 93 | loss: 0.15231 | val_roc_auc_c: 0.91596 |  0:33:11s\n",
      "epoch 94 | loss: 0.15216 | val_roc_auc_c: 0.91553 |  0:33:32s\n",
      "epoch 95 | loss: 0.15246 | val_roc_auc_c: 0.91522 |  0:33:53s\n",
      "epoch 96 | loss: 0.15244 | val_roc_auc_c: 0.9161  |  0:34:14s\n",
      "epoch 97 | loss: 0.15227 | val_roc_auc_c: 0.91592 |  0:34:35s\n",
      "epoch 98 | loss: 0.15232 | val_roc_auc_c: 0.91657 |  0:34:56s\n",
      "epoch 99 | loss: 0.15214 | val_roc_auc_c: 0.91586 |  0:35:17s\n",
      "epoch 100| loss: 0.15231 | val_roc_auc_c: 0.9162  |  0:35:38s\n",
      "epoch 101| loss: 0.15231 | val_roc_auc_c: 0.91545 |  0:35:59s\n",
      "epoch 102| loss: 0.1519  | val_roc_auc_c: 0.91579 |  0:36:20s\n",
      "epoch 103| loss: 0.1522  | val_roc_auc_c: 0.9159  |  0:36:40s\n",
      "epoch 104| loss: 0.15225 | val_roc_auc_c: 0.91563 |  0:37:02s\n",
      "epoch 105| loss: 0.15293 | val_roc_auc_c: 0.91579 |  0:37:23s\n",
      "epoch 106| loss: 0.15225 | val_roc_auc_c: 0.91587 |  0:37:43s\n",
      "epoch 107| loss: 0.15257 | val_roc_auc_c: 0.91527 |  0:38:05s\n",
      "epoch 108| loss: 0.15201 | val_roc_auc_c: 0.9157  |  0:38:26s\n",
      "epoch 109| loss: 0.15305 | val_roc_auc_c: 0.91586 |  0:38:46s\n",
      "epoch 110| loss: 0.15206 | val_roc_auc_c: 0.91598 |  0:39:07s\n",
      "epoch 111| loss: 0.1519  | val_roc_auc_c: 0.91535 |  0:39:27s\n",
      "epoch 112| loss: 0.15198 | val_roc_auc_c: 0.91567 |  0:39:48s\n",
      "epoch 113| loss: 0.15247 | val_roc_auc_c: 0.91544 |  0:40:09s\n",
      "epoch 114| loss: 0.15216 | val_roc_auc_c: 0.91589 |  0:40:30s\n",
      "epoch 115| loss: 0.15163 | val_roc_auc_c: 0.91557 |  0:40:50s\n",
      "epoch 116| loss: 0.15264 | val_roc_auc_c: 0.91542 |  0:41:11s\n",
      "epoch 117| loss: 0.15232 | val_roc_auc_c: 0.91624 |  0:41:31s\n",
      "epoch 118| loss: 0.15257 | val_roc_auc_c: 0.91585 |  0:41:53s\n",
      "\n",
      "Early stopping occured at epoch 118 with best_epoch = 98 and best_val_roc_auc_c = 0.91657\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n",
      "\u001b[34m FOLDS:  \u001b[31m 8\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.40248 | val_roc_auc_c: 0.5385  |  0:00:20s\n",
      "epoch 1  | loss: 0.35906 | val_roc_auc_c: 0.71294 |  0:00:42s\n",
      "epoch 2  | loss: 0.33502 | val_roc_auc_c: 0.81335 |  0:01:04s\n",
      "epoch 3  | loss: 0.3063  | val_roc_auc_c: 0.86691 |  0:01:26s\n",
      "epoch 4  | loss: 0.28045 | val_roc_auc_c: 0.88536 |  0:01:48s\n",
      "epoch 5  | loss: 0.26206 | val_roc_auc_c: 0.89176 |  0:02:10s\n",
      "epoch 6  | loss: 0.24824 | val_roc_auc_c: 0.89499 |  0:02:32s\n",
      "epoch 7  | loss: 0.2397  | val_roc_auc_c: 0.89861 |  0:02:54s\n",
      "epoch 8  | loss: 0.23308 | val_roc_auc_c: 0.89969 |  0:03:15s\n",
      "epoch 9  | loss: 0.22763 | val_roc_auc_c: 0.90137 |  0:03:36s\n",
      "epoch 10 | loss: 0.22266 | val_roc_auc_c: 0.90234 |  0:03:58s\n",
      "epoch 11 | loss: 0.21771 | val_roc_auc_c: 0.90176 |  0:04:19s\n",
      "epoch 12 | loss: 0.21412 | val_roc_auc_c: 0.90289 |  0:04:40s\n",
      "epoch 13 | loss: 0.21    | val_roc_auc_c: 0.90479 |  0:05:02s\n",
      "epoch 14 | loss: 0.20718 | val_roc_auc_c: 0.90728 |  0:05:24s\n",
      "epoch 15 | loss: 0.20421 | val_roc_auc_c: 0.90598 |  0:05:45s\n",
      "epoch 16 | loss: 0.20097 | val_roc_auc_c: 0.90759 |  0:06:06s\n",
      "epoch 17 | loss: 0.19913 | val_roc_auc_c: 0.90871 |  0:06:28s\n",
      "epoch 18 | loss: 0.1956  | val_roc_auc_c: 0.90977 |  0:06:49s\n",
      "epoch 19 | loss: 0.19233 | val_roc_auc_c: 0.90932 |  0:07:10s\n",
      "epoch 20 | loss: 0.19164 | val_roc_auc_c: 0.90768 |  0:07:31s\n",
      "epoch 21 | loss: 0.18903 | val_roc_auc_c: 0.90852 |  0:07:52s\n",
      "epoch 22 | loss: 0.18678 | val_roc_auc_c: 0.90997 |  0:08:14s\n",
      "epoch 23 | loss: 0.18553 | val_roc_auc_c: 0.91085 |  0:08:35s\n",
      "epoch 24 | loss: 0.18404 | val_roc_auc_c: 0.91062 |  0:08:57s\n",
      "epoch 25 | loss: 0.18207 | val_roc_auc_c: 0.90946 |  0:09:18s\n",
      "epoch 26 | loss: 0.18057 | val_roc_auc_c: 0.91152 |  0:09:39s\n",
      "epoch 27 | loss: 0.17845 | val_roc_auc_c: 0.91099 |  0:10:00s\n",
      "epoch 28 | loss: 0.17788 | val_roc_auc_c: 0.91016 |  0:10:22s\n",
      "epoch 29 | loss: 0.17749 | val_roc_auc_c: 0.9121  |  0:10:43s\n",
      "epoch 30 | loss: 0.17466 | val_roc_auc_c: 0.91084 |  0:11:04s\n",
      "epoch 31 | loss: 0.17368 | val_roc_auc_c: 0.91093 |  0:11:26s\n",
      "epoch 32 | loss: 0.17295 | val_roc_auc_c: 0.9118  |  0:11:47s\n",
      "epoch 33 | loss: 0.17148 | val_roc_auc_c: 0.90999 |  0:12:08s\n",
      "Epoch    34: reducing learning rate of group 0 to 6.6000e-04.\n",
      "epoch 34 | loss: 0.16551 | val_roc_auc_c: 0.91396 |  0:12:29s\n",
      "epoch 35 | loss: 0.16372 | val_roc_auc_c: 0.91483 |  0:12:50s\n",
      "epoch 36 | loss: 0.16128 | val_roc_auc_c: 0.91501 |  0:13:12s\n",
      "epoch 37 | loss: 0.15992 | val_roc_auc_c: 0.91439 |  0:13:33s\n",
      "epoch 38 | loss: 0.15913 | val_roc_auc_c: 0.9153  |  0:13:54s\n",
      "epoch 39 | loss: 0.15806 | val_roc_auc_c: 0.91479 |  0:14:16s\n",
      "epoch 40 | loss: 0.1579  | val_roc_auc_c: 0.91414 |  0:14:37s\n",
      "epoch 41 | loss: 0.15717 | val_roc_auc_c: 0.91371 |  0:14:59s\n",
      "epoch 42 | loss: 0.15696 | val_roc_auc_c: 0.91433 |  0:15:21s\n",
      "Epoch    43: reducing learning rate of group 0 to 2.1780e-04.\n",
      "epoch 43 | loss: 0.15491 | val_roc_auc_c: 0.9157  |  0:15:42s\n",
      "epoch 44 | loss: 0.15414 | val_roc_auc_c: 0.91595 |  0:16:04s\n",
      "epoch 45 | loss: 0.15405 | val_roc_auc_c: 0.91646 |  0:16:26s\n",
      "epoch 46 | loss: 0.1528  | val_roc_auc_c: 0.91575 |  0:16:47s\n",
      "epoch 47 | loss: 0.15303 | val_roc_auc_c: 0.91594 |  0:17:09s\n",
      "epoch 48 | loss: 0.15232 | val_roc_auc_c: 0.91553 |  0:17:30s\n",
      "epoch 49 | loss: 0.152   | val_roc_auc_c: 0.91539 |  0:17:51s\n",
      "Epoch    50: reducing learning rate of group 0 to 7.1874e-05.\n",
      "epoch 50 | loss: 0.15196 | val_roc_auc_c: 0.91611 |  0:18:12s\n",
      "epoch 51 | loss: 0.1509  | val_roc_auc_c: 0.9159  |  0:18:33s\n",
      "epoch 52 | loss: 0.15122 | val_roc_auc_c: 0.91527 |  0:18:55s\n",
      "epoch 53 | loss: 0.15108 | val_roc_auc_c: 0.91531 |  0:19:16s\n",
      "Epoch    54: reducing learning rate of group 0 to 2.3718e-05.\n",
      "epoch 54 | loss: 0.15134 | val_roc_auc_c: 0.91514 |  0:19:37s\n",
      "epoch 55 | loss: 0.15065 | val_roc_auc_c: 0.91558 |  0:19:59s\n",
      "epoch 56 | loss: 0.15088 | val_roc_auc_c: 0.91523 |  0:20:20s\n",
      "epoch 57 | loss: 0.1512  | val_roc_auc_c: 0.91517 |  0:20:42s\n",
      "Epoch    58: reducing learning rate of group 0 to 7.8271e-06.\n",
      "epoch 58 | loss: 0.15083 | val_roc_auc_c: 0.91536 |  0:21:03s\n",
      "epoch 59 | loss: 0.15019 | val_roc_auc_c: 0.9156  |  0:21:24s\n",
      "epoch 60 | loss: 0.15013 | val_roc_auc_c: 0.91579 |  0:21:45s\n",
      "epoch 61 | loss: 0.15084 | val_roc_auc_c: 0.91563 |  0:22:07s\n",
      "Epoch    62: reducing learning rate of group 0 to 2.5829e-06.\n",
      "epoch 62 | loss: 0.15016 | val_roc_auc_c: 0.91539 |  0:22:28s\n",
      "epoch 63 | loss: 0.15052 | val_roc_auc_c: 0.91565 |  0:22:50s\n",
      "epoch 64 | loss: 0.1502  | val_roc_auc_c: 0.91546 |  0:23:11s\n",
      "epoch 65 | loss: 0.1511  | val_roc_auc_c: 0.91532 |  0:23:32s\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "\n",
      "Early stopping occured at epoch 65 with best_epoch = 45 and best_val_roc_auc_c = 0.91646\n",
      "Best weights from best epoch are automatically used!\n",
      "\u001b[33m ------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "scores_auc_all = []\n",
    "test_cv_preds = []\n",
    "\n",
    "NB_SPLITS = 8 # 7\n",
    "mskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\n",
    "\n",
    "oof_preds = []\n",
    "oof_targets = []\n",
    "scores = []\n",
    "scores_auc = []\n",
    "\n",
    "# for mskf\n",
    "ms_tar = np.hstack((np.array(train_df),np.array(targets).reshape(-1,1)))\n",
    "#####\n",
    "\n",
    "for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train_df, ms_tar)):\n",
    "    print(b_,\"FOLDS: \", r_, fold_nb + 1)\n",
    "    print(g_, '*' * 60, c_)\n",
    "    \n",
    "    X_train, y_train = train_df.values[train_idx, :], np.array(targets).reshape(-1,1)[train_idx]\n",
    "    X_val, y_val = train_df.values[val_idx, :], np.array(targets).reshape(-1,1)[val_idx]\n",
    "    ### Model ###\n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "        \n",
    "    ### Fit ###\n",
    "    # Another change to the original code\n",
    "    # virtual_batch_size of 32 instead of 128\n",
    "    model.fit(\n",
    "        X_train = X_train,\n",
    "        y_train = y_train,\n",
    "        eval_set = [(X_val, y_val)],\n",
    "        eval_name = [\"val\"],\n",
    "        eval_metric = [\"roc_auc_c\"],\n",
    "        max_epochs = MAX_EPOCH,\n",
    "        patience = 20,\n",
    "        batch_size = 1024, \n",
    "        virtual_batch_size = 32,\n",
    "        num_workers = 4,\n",
    "        drop_last = False,\n",
    "        # To use binary cross entropy because this is not a regression problem\n",
    "        loss_fn = F.binary_cross_entropy_with_logits\n",
    "    )\n",
    "    print(y_, '-' * 60)\n",
    "    \n",
    "    ### Predict on validation ###\n",
    "    preds_val = model.predict(X_val)\n",
    "    # Apply sigmoid to the predictions\n",
    "    preds = 1 / (1 + np.exp(-preds_val))\n",
    "    score = np.min(model.history[\"val_roc_auc_c\"])\n",
    "    \n",
    "    ### Save OOF for CV ###\n",
    "    oof_preds.append(preds)\n",
    "    oof_targets.append(y_val)\n",
    "    scores.append(score)\n",
    "    \n",
    "    ### Predict on test ###\n",
    "    preds_test = model.predict(X_test)\n",
    "    test_cv_preds.append(1 / (1 + np.exp(-preds_test)))\n",
    "\n",
    "oof_preds_all = np.concatenate(oof_preds)\n",
    "oof_targets_all = np.concatenate(oof_targets)\n",
    "test_preds_all = np.stack(test_cv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T13:16:10.575615Z",
     "iopub.status.busy": "2021-04-16T13:16:10.574617Z",
     "iopub.status.idle": "2021-04-16T13:16:10.622613Z",
     "shell.execute_reply": "2021-04-16T13:16:10.622147Z"
    },
    "papermill": {
     "duration": 0.302444,
     "end_time": "2021-04-16T13:16:10.622714",
     "exception": false,
     "start_time": "2021-04-16T13:16:10.320270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mOverall AUC: \u001b[31m0.8072572868857321\n",
      "\u001b[34mAverage CV: \u001b[31m0.5369531970289722\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "for task_id in range(oof_preds_all.shape[1]):\n",
    "    aucs.append(roc_auc_score(y_true = oof_targets_all[:, task_id],\n",
    "                              y_score = oof_preds_all[:, task_id].round()\n",
    "                             ))\n",
    "print(f\"{b_}Overall AUC: {r_}{np.mean(aucs)}\")\n",
    "print(f\"{b_}Average CV: {r_}{np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.255351,
     "end_time": "2021-04-16T13:16:11.123174",
     "exception": false,
     "start_time": "2021-04-16T13:16:10.867823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**The worst CV value that i achive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.247833,
     "end_time": "2021-04-16T13:16:11.615972",
     "exception": false,
     "start_time": "2021-04-16T13:16:11.368139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font color = \"seagreen\">Conclusion (NOT AVAILABLE UNTIL I SEE THE LB Score)</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.245601,
     "end_time": "2021-04-16T13:16:12.106829",
     "exception": false,
     "start_time": "2021-04-16T13:16:11.861228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font color = \"seagreen\">Submission</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T13:16:12.615140Z",
     "iopub.status.busy": "2021-04-16T13:16:12.614316Z",
     "iopub.status.idle": "2021-04-16T13:16:12.617570Z",
     "shell.execute_reply": "2021-04-16T13:16:12.617119Z"
    },
    "papermill": {
     "duration": 0.256821,
     "end_time": "2021-04-16T13:16:12.617658",
     "exception": false,
     "start_time": "2021-04-16T13:16:12.360837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tempo = np.zeros((test_preds_all.shape[1],1))\n",
    "for i in range(test_preds_all.shape[0]):\n",
    "    tempo += test_preds_all[i]\n",
    "\n",
    "tempo /= test_preds_all.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T13:16:13.119110Z",
     "iopub.status.busy": "2021-04-16T13:16:13.118172Z",
     "iopub.status.idle": "2021-04-16T13:16:13.546710Z",
     "shell.execute_reply": "2021-04-16T13:16:13.546113Z"
    },
    "papermill": {
     "duration": 0.682477,
     "end_time": "2021-04-16T13:16:13.546923",
     "exception": false,
     "start_time": "2021-04-16T13:16:12.864446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>risk_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  risk_flag\n",
       "0   1          0\n",
       "1   2          0\n",
       "2   3          0\n",
       "3   4          0\n",
       "4   5          0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#submission[all_feat] = tmp.mean(axis = 0)\n",
    "\n",
    "# Set control to 0\n",
    "#submission.loc[test[\"cp_type\"] == 0, submission.columns[1:]] = 0\n",
    "submission['risk_flag'] = tempo.round().reshape(-1,).astype(np.int64)\n",
    "submission.to_csv(\"submission.csv\", index = False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T13:16:14.049975Z",
     "iopub.status.busy": "2021-04-16T13:16:14.049343Z",
     "iopub.status.idle": "2021-04-16T13:16:14.052303Z",
     "shell.execute_reply": "2021-04-16T13:16:14.052845Z"
    },
    "papermill": {
     "duration": 0.255144,
     "end_time": "2021-04-16T13:16:14.052973",
     "exception": false,
     "start_time": "2021-04-16T13:16:13.797829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34msubmission.shape: \u001b[31m(28000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{b_}submission.shape: {r_}{submission.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.247915,
     "end_time": "2021-04-16T13:16:14.553361",
     "exception": false,
     "start_time": "2021-04-16T13:16:14.305446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class = \"alert alert-block alert-info\">\n",
    "    <h3><font color = \"red\">NOTE: </font></h3>\n",
    "    <p>If you want to comment please tag me with '@' to answer more quickly.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.247888,
     "end_time": "2021-04-16T13:16:15.048798",
     "exception": false,
     "start_time": "2021-04-16T13:16:14.800910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 16332.243549,
   "end_time": "2021-04-16T13:16:16.495570",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-16T08:44:04.252021",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
